{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gradient.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPQPS7TE4cqRqlW2Ct2U50Q"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"IYTQZjh3YKeR"},"source":["import numpy as np\r\n","\r\n","def _numerical_gradient_1d(f, x):\r\n","    h = 1e-4 # 0.0001\r\n","    grad = np.zeros_like(x)\r\n","    \r\n","    for idx in range(x.size):\r\n","        tmp_val = x[idx]\r\n","        x[idx] = float(tmp_val) + h\r\n","        fxh1 = f(x) # f(x+h)\r\n","        \r\n","        x[idx] = tmp_val - h \r\n","        fxh2 = f(x) # f(x-h)\r\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\r\n","        \r\n","        x[idx] = tmp_val # 값 복원\r\n","        \r\n","    return grad\r\n","\r\n","\r\n","def numerical_gradient_2d(f, X):\r\n","    if X.ndim == 1:\r\n","        return _numerical_gradient_1d(f, X)\r\n","    else:\r\n","        grad = np.zeros_like(X)\r\n","        \r\n","        for idx, x in enumerate(X):\r\n","            grad[idx] = _numerical_gradient_1d(f, x)\r\n","        \r\n","        return grad\r\n","\r\n","\r\n","def numerical_gradient(f, x):\r\n","    h = 1e-4 # 0.0001\r\n","    grad = np.zeros_like(x)\r\n","    \r\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\r\n","    while not it.finished:\r\n","        idx = it.multi_index\r\n","        tmp_val = x[idx]\r\n","        x[idx] = float(tmp_val) + h\r\n","        fxh1 = f(x) # f(x+h)\r\n","        \r\n","        x[idx] = tmp_val - h \r\n","        fxh2 = f(x) # f(x-h)\r\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\r\n","        \r\n","        x[idx] = tmp_val # 값 복원\r\n","        it.iternext()   \r\n","        \r\n","    return grad"],"execution_count":null,"outputs":[]}]}