{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trainer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN2T0UEh/GkefH7ZbqI3U1e"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"R-uJWTlWZBRH"},"source":["# coding: utf-8\r\n","import sys, os\r\n","sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n","import numpy as np\r\n","from common.optimizer import *\r\n","\r\n","class Trainer:\r\n","    \"\"\"신경망 훈련을 대신 해주는 클래스\r\n","    \"\"\"\r\n","    def __init__(self, network, x_train, t_train, x_test, t_test,\r\n","                 epochs=20, mini_batch_size=100,\r\n","                 optimizer='SGD', optimizer_param={'lr':0.01}, \r\n","                 evaluate_sample_num_per_epoch=None, verbose=True):\r\n","        self.network = network\r\n","        self.verbose = verbose\r\n","        self.x_train = x_train\r\n","        self.t_train = t_train\r\n","        self.x_test = x_test\r\n","        self.t_test = t_test\r\n","        self.epochs = epochs\r\n","        self.batch_size = mini_batch_size\r\n","        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\r\n","\r\n","        # optimzer\r\n","        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\r\n","                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\r\n","        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\r\n","        \r\n","        self.train_size = x_train.shape[0]\r\n","        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\r\n","        self.max_iter = int(epochs * self.iter_per_epoch)\r\n","        self.current_iter = 0\r\n","        self.current_epoch = 0\r\n","        \r\n","        self.train_loss_list = []\r\n","        self.train_acc_list = []\r\n","        self.test_acc_list = []\r\n","\r\n","    def train_step(self):\r\n","        batch_mask = np.random.choice(self.train_size, self.batch_size)\r\n","        x_batch = self.x_train[batch_mask]\r\n","        t_batch = self.t_train[batch_mask]\r\n","        \r\n","        grads = self.network.gradient(x_batch, t_batch)\r\n","        self.optimizer.update(self.network.params, grads)\r\n","        \r\n","        loss = self.network.loss(x_batch, t_batch)\r\n","        self.train_loss_list.append(loss)\r\n","        if self.verbose: print(\"train loss:\" + str(loss))\r\n","        \r\n","        if self.current_iter % self.iter_per_epoch == 0:\r\n","            self.current_epoch += 1\r\n","            \r\n","            x_train_sample, t_train_sample = self.x_train, self.t_train\r\n","            x_test_sample, t_test_sample = self.x_test, self.t_test\r\n","            if not self.evaluate_sample_num_per_epoch is None:\r\n","                t = self.evaluate_sample_num_per_epoch\r\n","                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\r\n","                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\r\n","                \r\n","            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\r\n","            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\r\n","            self.train_acc_list.append(train_acc)\r\n","            self.test_acc_list.append(test_acc)\r\n","\r\n","            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\r\n","        self.current_iter += 1\r\n","\r\n","    def train(self):\r\n","        for i in range(self.max_iter):\r\n","            self.train_step()\r\n","\r\n","        test_acc = self.network.accuracy(self.x_test, self.t_test)\r\n","\r\n","        if self.verbose:\r\n","            print(\"=============== Final Test Accuracy ===============\")\r\n","            print(\"test acc:\" + str(test_acc))"],"execution_count":null,"outputs":[]}]}