{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multi_layer_net.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNK+FYIUKD5fprEhXIRn5F"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lK3rQU5oYmrW"},"source":["# coding: utf-8\r\n","import sys, os\r\n","sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n","import numpy as np\r\n","from collections import OrderedDict\r\n","from common.layers import *\r\n","from common.gradient import numerical_gradient\r\n","\r\n","\r\n","class MultiLayerNet:\r\n","    \"\"\"완전연결 다층 신경망\r\n","    Parameters\r\n","    ----------\r\n","    input_size : 입력 크기（MNIST의 경우엔 784）\r\n","    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\r\n","    output_size : 출력 크기（MNIST의 경우엔 10）\r\n","    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\r\n","    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\r\n","        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\r\n","        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\r\n","    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\r\n","    \"\"\"\r\n","    def __init__(self, input_size, hidden_size_list, output_size,\r\n","                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\r\n","        self.input_size = input_size\r\n","        self.output_size = output_size\r\n","        self.hidden_size_list = hidden_size_list\r\n","        self.hidden_layer_num = len(hidden_size_list)\r\n","        self.weight_decay_lambda = weight_decay_lambda\r\n","        self.params = {}\r\n","\r\n","        # 가중치 초기화\r\n","        self.__init_weight(weight_init_std)\r\n","\r\n","        # 계층 생성\r\n","        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\r\n","        self.layers = OrderedDict()\r\n","        for idx in range(1, self.hidden_layer_num+1):\r\n","            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\r\n","                                                      self.params['b' + str(idx)])\r\n","            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\r\n","\r\n","        idx = self.hidden_layer_num + 1\r\n","        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\r\n","            self.params['b' + str(idx)])\r\n","\r\n","        self.last_layer = SoftmaxWithLoss()\r\n","\r\n","    def __init_weight(self, weight_init_std):\r\n","        \"\"\"가중치 초기화\r\n","        \r\n","        Parameters\r\n","        ----------\r\n","        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\r\n","            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\r\n","            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\r\n","        \"\"\"\r\n","        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\r\n","        for idx in range(1, len(all_size_list)):\r\n","            scale = weight_init_std\r\n","            if str(weight_init_std).lower() in ('relu', 'he'):\r\n","                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\r\n","            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\r\n","                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\r\n","            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\r\n","            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\r\n","\r\n","    def predict(self, x):\r\n","        for layer in self.layers.values():\r\n","            x = layer.forward(x)\r\n","\r\n","        return x\r\n","\r\n","    def loss(self, x, t):\r\n","        \"\"\"손실 함수를 구한다.\r\n","        \r\n","        Parameters\r\n","        ----------\r\n","        x : 입력 데이터\r\n","        t : 정답 레이블 \r\n","        \r\n","        Returns\r\n","        -------\r\n","        손실 함수의 값\r\n","        \"\"\"\r\n","        y = self.predict(x)\r\n","\r\n","        weight_decay = 0\r\n","        for idx in range(1, self.hidden_layer_num + 2):\r\n","            W = self.params['W' + str(idx)]\r\n","            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\r\n","\r\n","        return self.last_layer.forward(y, t) + weight_decay\r\n","\r\n","    def accuracy(self, x, t):\r\n","        y = self.predict(x)\r\n","        y = np.argmax(y, axis=1)\r\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\r\n","\r\n","        accuracy = np.sum(y == t) / float(x.shape[0])\r\n","        return accuracy\r\n","\r\n","    def numerical_gradient(self, x, t):\r\n","        \"\"\"기울기를 구한다(수치 미분).\r\n","        \r\n","        Parameters\r\n","        ----------\r\n","        x : 입력 데이터\r\n","        t : 정답 레이블\r\n","        \r\n","        Returns\r\n","        -------\r\n","        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\r\n","            grads['W1']、grads['W2']、... 각 층의 가중치\r\n","            grads['b1']、grads['b2']、... 각 층의 편향\r\n","        \"\"\"\r\n","        loss_W = lambda W: self.loss(x, t)\r\n","\r\n","        grads = {}\r\n","        for idx in range(1, self.hidden_layer_num+2):\r\n","            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\r\n","            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\r\n","\r\n","        return grads\r\n","\r\n","    def gradient(self, x, t):\r\n","        \"\"\"기울기를 구한다(오차역전파법).\r\n","        Parameters\r\n","        ----------\r\n","        x : 입력 데이터\r\n","        t : 정답 레이블\r\n","        \r\n","        Returns\r\n","        -------\r\n","        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\r\n","            grads['W1']、grads['W2']、... 각 층의 가중치\r\n","            grads['b1']、grads['b2']、... 각 층의 편향\r\n","        \"\"\"\r\n","        # forward\r\n","        self.loss(x, t)\r\n","\r\n","        # backward\r\n","        dout = 1\r\n","        dout = self.last_layer.backward(dout)\r\n","\r\n","        layers = list(self.layers.values())\r\n","        layers.reverse()\r\n","        for layer in layers:\r\n","            dout = layer.backward(dout)\r\n","\r\n","        # 결과 저장\r\n","        grads = {}\r\n","        for idx in range(1, self.hidden_layer_num+2):\r\n","            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\r\n","            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\r\n","\r\n","        return grads"],"execution_count":null,"outputs":[]}]}