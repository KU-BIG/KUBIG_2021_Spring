{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"optimizer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPH6TiZR27Xd5C8MPYur0jZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"y3_OkTYCY5CG"},"source":["# coding: utf-8\r\n","import numpy as np\r\n","\r\n","class SGD:\r\n","\r\n","    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\r\n","\r\n","    def __init__(self, lr=0.01):\r\n","        self.lr = lr\r\n","        \r\n","    def update(self, params, grads):\r\n","        for key in params.keys():\r\n","            params[key] -= self.lr * grads[key] \r\n","\r\n","\r\n","class Momentum:\r\n","\r\n","    \"\"\"모멘텀 SGD\"\"\"\r\n","\r\n","    def __init__(self, lr=0.01, momentum=0.9):\r\n","        self.lr = lr\r\n","        self.momentum = momentum\r\n","        self.v = None\r\n","        \r\n","    def update(self, params, grads):\r\n","        if self.v is None:\r\n","            self.v = {}\r\n","            for key, val in params.items():                                \r\n","                self.v[key] = np.zeros_like(val)\r\n","                \r\n","        for key in params.keys():\r\n","            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \r\n","            params[key] += self.v[key]\r\n","\r\n","\r\n","class Nesterov:\r\n","\r\n","    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\r\n","    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\r\n","    \r\n","    def __init__(self, lr=0.01, momentum=0.9):\r\n","        self.lr = lr\r\n","        self.momentum = momentum\r\n","        self.v = None\r\n","        \r\n","    def update(self, params, grads):\r\n","        if self.v is None:\r\n","            self.v = {}\r\n","            for key, val in params.items():\r\n","                self.v[key] = np.zeros_like(val)\r\n","            \r\n","        for key in params.keys():\r\n","            self.v[key] *= self.momentum\r\n","            self.v[key] -= self.lr * grads[key]\r\n","            params[key] += self.momentum * self.momentum * self.v[key]\r\n","            params[key] -= (1 + self.momentum) * self.lr * grads[key]\r\n","\r\n","\r\n","class AdaGrad:\r\n","\r\n","    \"\"\"AdaGrad\"\"\"\r\n","\r\n","    def __init__(self, lr=0.01):\r\n","        self.lr = lr\r\n","        self.h = None\r\n","        \r\n","    def update(self, params, grads):\r\n","        if self.h is None:\r\n","            self.h = {}\r\n","            for key, val in params.items():\r\n","                self.h[key] = np.zeros_like(val)\r\n","            \r\n","        for key in params.keys():\r\n","            self.h[key] += grads[key] * grads[key]\r\n","            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\r\n","\r\n","\r\n","class RMSprop:\r\n","\r\n","    \"\"\"RMSprop\"\"\"\r\n","\r\n","    def __init__(self, lr=0.01, decay_rate = 0.99):\r\n","        self.lr = lr\r\n","        self.decay_rate = decay_rate\r\n","        self.h = None\r\n","        \r\n","    def update(self, params, grads):\r\n","        if self.h is None:\r\n","            self.h = {}\r\n","            for key, val in params.items():\r\n","                self.h[key] = np.zeros_like(val)\r\n","            \r\n","        for key in params.keys():\r\n","            self.h[key] *= self.decay_rate\r\n","            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\r\n","            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\r\n","\r\n","\r\n","class Adam:\r\n","\r\n","    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\r\n","\r\n","    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\r\n","        self.lr = lr\r\n","        self.beta1 = beta1\r\n","        self.beta2 = beta2\r\n","        self.iter = 0\r\n","        self.m = None\r\n","        self.v = None\r\n","        \r\n","    def update(self, params, grads):\r\n","        if self.m is None:\r\n","            self.m, self.v = {}, {}\r\n","            for key, val in params.items():\r\n","                self.m[key] = np.zeros_like(val)\r\n","                self.v[key] = np.zeros_like(val)\r\n","        \r\n","        self.iter += 1\r\n","        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \r\n","        \r\n","        for key in params.keys():\r\n","            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\r\n","            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\r\n","            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\r\n","            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\r\n","            \r\n","            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\r\n","            \r\n","            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\r\n","            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\r\n","            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"],"execution_count":null,"outputs":[]}]}