{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multi_layer_net_extend.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNpESts/SVwPEZGyYF2Y2Tp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1rY27-JwYsza"},"source":["# coding: utf-8\r\n","import sys, os\r\n","sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정\r\n","import numpy as np\r\n","from collections import OrderedDict\r\n","from common.layers import *\r\n","from common.gradient import numerical_gradient\r\n","\r\n","class MultiLayerNetExtend:\r\n","    \"\"\"완전 연결 다층 신경망(확장판)\r\n","    가중치 감소, 드롭아웃, 배치 정규화 구현\r\n","    Parameters\r\n","    ----------\r\n","    input_size : 입력 크기（MNIST의 경우엔 784）\r\n","    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\r\n","    output_size : 출력 크기（MNIST의 경우엔 10）\r\n","    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\r\n","    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\r\n","        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\r\n","        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\r\n","    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\r\n","    use_dropout : 드롭아웃 사용 여부\r\n","    dropout_ration : 드롭아웃 비율\r\n","    use_batchNorm : 배치 정규화 사용 여부\r\n","    \"\"\"\r\n","    def __init__(self, input_size, hidden_size_list, output_size,\r\n","                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \r\n","                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False):\r\n","        self.input_size = input_size\r\n","        self.output_size = output_size\r\n","        self.hidden_size_list = hidden_size_list\r\n","        self.hidden_layer_num = len(hidden_size_list)\r\n","        self.use_dropout = use_dropout\r\n","        self.weight_decay_lambda = weight_decay_lambda\r\n","        self.use_batchnorm = use_batchnorm\r\n","        self.params = {}\r\n","\r\n","        # 가중치 초기화\r\n","        self.__init_weight(weight_init_std)\r\n","\r\n","        # 계층 생성\r\n","        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\r\n","        self.layers = OrderedDict()\r\n","        for idx in range(1, self.hidden_layer_num+1):\r\n","            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\r\n","                                                      self.params['b' + str(idx)])\r\n","            if self.use_batchnorm:\r\n","                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\r\n","                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\r\n","                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\r\n","                \r\n","            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\r\n","            \r\n","            if self.use_dropout:\r\n","                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\r\n","\r\n","        idx = self.hidden_layer_num + 1\r\n","        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\r\n","\r\n","        self.last_layer = SoftmaxWithLoss()\r\n","\r\n","    def __init_weight(self, weight_init_std):\r\n","        \"\"\"가중치 초기화\r\n","        \r\n","        Parameters\r\n","        ----------\r\n","        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\r\n","            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\r\n","            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\r\n","        \"\"\"\r\n","        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\r\n","        for idx in range(1, len(all_size_list)):\r\n","            scale = weight_init_std\r\n","            if str(weight_init_std).lower() in ('relu', 'he'):\r\n","                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\r\n","            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\r\n","                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\r\n","            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\r\n","            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\r\n","\r\n","    def predict(self, x, train_flg=False):\r\n","        for key, layer in self.layers.items():\r\n","            if \"Dropout\" in key or \"BatchNorm\" in key:\r\n","                x = layer.forward(x, train_flg)\r\n","            else:\r\n","                x = layer.forward(x)\r\n","\r\n","        return x\r\n","\r\n","    def loss(self, x, t, train_flg=False):\r\n","        \"\"\"손실 함수를 구한다.\r\n","        \r\n","        Parameters\r\n","        ----------\r\n","        x : 입력 데이터\r\n","        t : 정답 레이블 \r\n","        \"\"\"\r\n","        y = self.predict(x, train_flg)\r\n","\r\n","        weight_decay = 0\r\n","        for idx in range(1, self.hidden_layer_num + 2):\r\n","            W = self.params['W' + str(idx)]\r\n","            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\r\n","\r\n","        return self.last_layer.forward(y, t) + weight_decay\r\n","\r\n","    def accuracy(self, X, T):\r\n","        Y = self.predict(X, train_flg=False)\r\n","        Y = np.argmax(Y, axis=1)\r\n","        if T.ndim != 1 : T = np.argmax(T, axis=1)\r\n","\r\n","        accuracy = np.sum(Y == T) / float(X.shape[0])\r\n","        return accuracy\r\n","\r\n","    def numerical_gradient(self, X, T):\r\n","        \"\"\"기울기를 구한다(수치 미분).\r\n","        \r\n","        Parameters\r\n","        ----------\r\n","        x : 입력 데이터\r\n","        t : 정답 레이블\r\n","        \r\n","        Returns\r\n","        -------\r\n","        각 층의 기울기를 담은 사전(dictionary) 변수\r\n","            grads['W1']、grads['W2']、... 각 층의 가중치\r\n","            grads['b1']、grads['b2']、... 각 층의 편향\r\n","        \"\"\"\r\n","        loss_W = lambda W: self.loss(X, T, train_flg=True)\r\n","\r\n","        grads = {}\r\n","        for idx in range(1, self.hidden_layer_num+2):\r\n","            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\r\n","            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\r\n","            \r\n","            if self.use_batchnorm and idx != self.hidden_layer_num+1:\r\n","                grads['gamma' + str(idx)] = numerical_gradient(loss_W, self.params['gamma' + str(idx)])\r\n","                grads['beta' + str(idx)] = numerical_gradient(loss_W, self.params['beta' + str(idx)])\r\n","\r\n","        return grads\r\n","        \r\n","    def gradient(self, x, t):\r\n","        # forward\r\n","        self.loss(x, t, train_flg=True)\r\n","\r\n","        # backward\r\n","        dout = 1\r\n","        dout = self.last_layer.backward(dout)\r\n","\r\n","        layers = list(self.layers.values())\r\n","        layers.reverse()\r\n","        for layer in layers:\r\n","            dout = layer.backward(dout)\r\n","\r\n","        # 결과 저장\r\n","        grads = {}\r\n","        for idx in range(1, self.hidden_layer_num+2):\r\n","            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\r\n","            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\r\n","\r\n","            if self.use_batchnorm and idx != self.hidden_layer_num+1:\r\n","                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\r\n","                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\r\n","\r\n","        return grads"],"execution_count":null,"outputs":[]}]}