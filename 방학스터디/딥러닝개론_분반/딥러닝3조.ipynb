{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "people-Copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "46rVJMlxm6iZ"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/Shareddrives/NLP 대회/new')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbAQeqHQnLMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c067cb-760c-408e-bcd3-ee3d71337d6f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoCAyg97m6if",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525b768a-fcb8-4953-bfa2-7a4063a9c1e4"
      },
      "source": [
        "!pip install textstat\n",
        "!pip install fasttext\n",
        "!pip install xgboost\n",
        "!pip install nltk"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.10.0)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.0.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G9Oz_9Om6ig"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYoSJBMZm6ig"
      },
      "source": [
        "train = pd.read_csv('train.csv', encoding='utf-8')\n",
        "test = pd.read_csv('test_x.csv', encoding='utf-8')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHOpJr9Mm6ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bef14c1-0f4b-4f7d-c75d-142a3d8018a1"
      },
      "source": [
        "#사람 이름 구하기\n",
        "#필요 import\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import ne_chunk, tree2conlltags\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "#코드\n",
        "def get_persons(text):\n",
        "    a=ne_chunk(pos_tag(word_tokenize(text)))  \n",
        "    b=tree2conlltags(a)  \n",
        "    c=[list(x) for x in b]\n",
        "    #token화 하고 품사 태깅 후에 ne_chunk, tree2conlltags, 리스트 안에 리스트('단어','품사','사람여부*')가 들어가도록\n",
        "    #*원래는 사람여부는 아닌데 아래에서 PERSON 값을 가질 경우 따로 처리함\n",
        "    \n",
        "    \n",
        "    person_vector=[]\n",
        "    person_name=[]\n",
        "    for i, word in enumerate(c):\n",
        "        if(\"PERSON\" in word[2]):\n",
        "            person_name.append(word[0])\n",
        "        else:\n",
        "            if(len(person_name)!=0):\n",
        "                person_vector.append(\"\".join(person_name))\n",
        "            person_name=[]\n",
        "        \n",
        "        if(i==len(c)-1 and len(person_name)!=0):\n",
        "            person_vector.append(\"\".join(person_name))\n",
        "    return person_vector\n",
        "    #list에 번호를 매겨서 그 index를 i로 하고, 그 값들을 word라고 할 때, 만약 리스트('단어','품사','사람여부*')의 사람여부 항목이\n",
        "    #사람으로 표시되어 있으면, person_name에 그 단어를 추가한다.\n",
        "    #사람으로 표시되어 있지 않으면, person_name을 확인하여 비어있지 않으면 그 단어를 person_vector에 추가하고 person_name을 초기화\n",
        "    #만약 i(index)가 list의 길이-1 즉, 마지막 인덱스이고, person_name이 비어있지 않으면, 마지막 단어를 person_vector에 추가한다.\n",
        "    #최종 person_vector를 출력"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf9oMH6Lm6ih"
      },
      "source": [
        "#저자의 글 모으기\n",
        "# author번호가 *인 text를 쭉 늘어놓은 리스트\n",
        "text_author_0 = \" \".join(list(train['text'][train['author']==0]))\n",
        "text_author_1 = \" \".join(list(train['text'][train['author']==1]))\n",
        "text_author_2 = \" \".join(list(train['text'][train['author']==2]))\n",
        "text_author_3 = \" \".join(list(train['text'][train['author']==3]))\n",
        "text_author_4 = \" \".join(list(train['text'][train['author']==4]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Yi0457m6ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e475cf46-1c00-4f4c-b33c-d079cc434899"
      },
      "source": [
        "#저자의 글에 나오는 등장인물 이름\n",
        "# *번 저자의 글에 나오는 사람이름의 집합(순서 없음)\n",
        "persons_author_0 = set(get_persons(text_author_0))\n",
        "print(\"0 completed\")\n",
        "persons_author_1 = set(get_persons(text_author_1))\n",
        "print(\"1 completed\")\n",
        "persons_author_2 = set(get_persons(text_author_2))\n",
        "print(\"2 completed\")\n",
        "persons_author_3 = set(get_persons(text_author_3))\n",
        "print(\"3 completed\")\n",
        "persons_author_4 = set(get_persons(text_author_4))\n",
        "print(\"4 completed\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 completed\n",
            "1 completed\n",
            "2 completed\n",
            "3 completed\n",
            "4 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-d39i6xm6ih"
      },
      "source": [
        "#비트연산자를 이용한 함수\n",
        "# a&b 는 a의 이진수 값과 b의 이진수 값을 비교하여 \n",
        "# 자리수가 모두 1이면 1, 아니면 0을 output으로 하는 이진수의 십진수 값을 반환하는 연산자 (and와 유사)\n",
        "\n",
        "# a|b 는 a의 이진수 값과 b의 이진수 값을 비교하여 \n",
        "# 자리수가 하나라도 1이면 1, 아니면 0을 output으로 하는 이진수의 십진수 값을 반환하는 연산자 (or과 유사)\n",
        "\n",
        "# 아래 함수는 두 input의 교집합의 크기를 합집합의 크기로 나눠주는 함수. 항상 <=1\n",
        "\n",
        "\n",
        "def jaccard(a,b):\n",
        "    return len(a&b)/len(a|b)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lrRkD4Mm6ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938b5752-c4bf-4c91-86bd-06017db98c53"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# train과 test 데이터셋에서 각 행의 'text'에 나오는 사람이름이 *번 저자의 글에 나오는 사람이름과 얼마나 유사한지 측정하는 변수\n",
        "# 두 세트의 겹치는 이름 개수 / 두 텍스트 전체 이름 개수\n",
        "\n",
        "train[\"persons_0\"]=train[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "100%|██████████| 54879/54879 [15:02<00:00, 60.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cx9987Nm6ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41406ee-9b51-440b-9d59-f2519227a74d"
      },
      "source": [
        "train[\"persons_1\"]=train[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
        "train[\"persons_2\"]=train[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
        "train[\"persons_3\"]=train[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
        "train[\"persons_4\"]=train[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) \n",
        "\n",
        "test[\"persons_0\"]=test[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n",
        "test[\"persons_1\"]=test[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
        "test[\"persons_2\"]=test[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
        "test[\"persons_3\"]=test[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
        "test[\"persons_4\"]=test[\"text\"].progress_apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 54879/54879 [15:03<00:00, 60.72it/s]\n",
            "100%|██████████| 54879/54879 [15:15<00:00, 59.98it/s]\n",
            "100%|██████████| 54879/54879 [15:04<00:00, 60.70it/s]\n",
            "100%|██████████| 54879/54879 [15:17<00:00, 59.82it/s]\n",
            "100%|██████████| 19617/19617 [10:34<00:00, 30.90it/s]\n",
            "100%|██████████| 19617/19617 [10:36<00:00, 30.80it/s]\n",
            "100%|██████████| 19617/19617 [10:43<00:00, 30.50it/s]\n",
            "100%|██████████| 19617/19617 [10:56<00:00, 29.89it/s]\n",
            "100%|██████████| 19617/19617 [11:02<00:00, 29.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4qN7KCam6ii"
      },
      "source": [
        "import string"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EeBz1pBm6ii"
      },
      "source": [
        "#stopwords\n",
        "from nltk.corpus import stopwords\n",
        "eng_stopwords = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBPp-uOOm6ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcaccf9d-d169-48f0-e973-2e3fc12e7ba1"
      },
      "source": [
        "train[\"num_words\"] = train[\"text\"].progress_apply(lambda x: len(str(x).split()))\n",
        "train[\"num_unique_words\"] = train[\"text\"].progress_apply(lambda x: len(set(str(x).split())))\n",
        "train[\"num_chars\"] = train[\"text\"].progress_apply(lambda x: len(str(x)))\n",
        "train[\"num_stopwords\"] = train[\"text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "#train[\"num_punctuations\"] =train['text'].progress_apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "train[\"num_words_upper\"] = train[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "train[\"mean_word_len\"] = train[\"text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "train[\"num_words_title\"] = train[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 54879/54879 [00:00<00:00, 230198.55it/s]\n",
            "100%|██████████| 54879/54879 [00:00<00:00, 123942.98it/s]\n",
            "100%|██████████| 54879/54879 [00:00<00:00, 568745.36it/s]\n",
            "100%|██████████| 54879/54879 [00:00<00:00, 97526.37it/s]\n",
            "100%|██████████| 54879/54879 [00:00<00:00, 145102.08it/s]\n",
            "100%|██████████| 54879/54879 [00:01<00:00, 32693.72it/s]\n",
            "100%|██████████| 54879/54879 [00:00<00:00, 139484.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyuX9FIMm6ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77c4b95-6427-4b9e-f957-94840f160bce"
      },
      "source": [
        "test[\"num_words\"] = test[\"text\"].progress_apply(lambda x: len(str(x).split()))\n",
        "test[\"num_unique_words\"] = test[\"text\"].progress_apply(lambda x: len(set(str(x).split())))\n",
        "test[\"num_chars\"] = test[\"text\"].progress_apply(lambda x: len(str(x)))\n",
        "test[\"num_stopwords\"] = test[\"text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "#test[\"num_punctuations\"] =test['text'].progress_apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "test[\"num_words_upper\"] = test[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "test[\"mean_word_len\"] = test[\"text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test[\"num_words_title\"] = test[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19617/19617 [00:00<00:00, 141929.27it/s]\n",
            "100%|██████████| 19617/19617 [00:00<00:00, 73141.64it/s]\n",
            "100%|██████████| 19617/19617 [00:00<00:00, 586442.64it/s]\n",
            "100%|██████████| 19617/19617 [00:00<00:00, 52859.90it/s]\n",
            "100%|██████████| 19617/19617 [00:00<00:00, 79046.73it/s]\n",
            "100%|██████████| 19617/19617 [00:00<00:00, 25206.60it/s]\n",
            "100%|██████████| 19617/19617 [00:00<00:00, 72244.79it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6PSlpM_m6ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bf54502-8c0c-4361-eaad-c4c5a78d8c03"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def sentiment_nltk(text):\n",
        "    res = SentimentIntensityAnalyzer().polarity_scores(text)\n",
        "    return res['compound']\n",
        "\n",
        "train[\"sentiment\"]=train[\"text\"].progress_apply(sentiment_nltk)\n",
        "test[\"sentiment\"]=test[\"text\"].progress_apply(sentiment_nltk)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n",
            "100%|██████████| 54879/54879 [06:55<00:00, 132.20it/s]\n",
            "100%|██████████| 19617/19617 [02:38<00:00, 123.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT5VqvXVm6ik"
      },
      "source": [
        "#자주 사용하는 품사 정보\n",
        "\n",
        "def fraction_noun(text):\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    if word_count==0:\n",
        "        return 0\n",
        "    else:\n",
        "        pos_list = nltk.pos_tag(text_splited)\n",
        "        noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
        "    \n",
        "        return (noun_count/word_count)\n",
        "\n",
        "def fraction_adj(text):\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    if word_count==0:\n",
        "        return 0\n",
        "    else:\n",
        "        pos_list = nltk.pos_tag(text_splited)\n",
        "        adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
        "    \n",
        "        return (adj_count/word_count)  \n",
        "\n",
        "def fraction_verbs(text):\n",
        "    text_splited = text.split(' ')\n",
        "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
        "    text_splited = [s for s in text_splited if s]\n",
        "    word_count = text_splited.__len__()\n",
        "    if word_count==0:\n",
        "        return 0\n",
        "    else:\n",
        "        pos_list = nltk.pos_tag(text_splited)\n",
        "        verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
        "    \n",
        "        return (verbs_count/word_count)  \n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8XTTHUDm6ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d43c3e0-10f9-4da3-d723-eb3308ebdbe5"
      },
      "source": [
        "train['noun'] = train[\"text\"].progress_apply(lambda x: fraction_noun(x))\n",
        "train['adj'] = train[\"text\"].progress_apply(lambda x: fraction_adj(x))\n",
        "train['verbs'] = train[\"text\"].progress_apply(lambda x: fraction_verbs(x))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 54879/54879 [01:45<00:00, 518.80it/s]\n",
            "100%|██████████| 54879/54879 [01:44<00:00, 526.18it/s]\n",
            "100%|██████████| 54879/54879 [01:43<00:00, 528.59it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pf182o5m6il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8184cdcf-4fbc-49fb-b4fc-6ab70063bd84"
      },
      "source": [
        "test['noun'] = test[\"text\"].progress_apply(lambda x: fraction_noun(x))\n",
        "test['adj'] = test[\"text\"].progress_apply(lambda x: fraction_adj(x))\n",
        "test['verbs'] = test[\"text\"].progress_apply(lambda x: fraction_verbs(x))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19617/19617 [01:14<00:00, 261.59it/s]\n",
            "100%|██████████| 19617/19617 [01:14<00:00, 262.81it/s]\n",
            "100%|██████████| 19617/19617 [01:13<00:00, 265.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwNhnWqOm6il"
      },
      "source": [
        "import re\n",
        "\n",
        "#clean\n",
        "def clean(train,test):\n",
        "    train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in train['text']]\n",
        "    test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in test['text']]\n",
        "    return train,test\n",
        "train, test = clean(train, test)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMsDBVbgm6il",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "66b579d2-e2c7-4485-ea33-648aaff2cb34"
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>persons_0</th>\n",
              "      <th>persons_1</th>\n",
              "      <th>persons_2</th>\n",
              "      <th>persons_3</th>\n",
              "      <th>persons_4</th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_stopwords</th>\n",
              "      <th>num_words_upper</th>\n",
              "      <th>mean_word_len</th>\n",
              "      <th>num_words_title</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>noun</th>\n",
              "      <th>adj</th>\n",
              "      <th>verbs</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>He was almost choking. There was so much, so m...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>46</td>\n",
              "      <td>39</td>\n",
              "      <td>240</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>4.239130</td>\n",
              "      <td>4</td>\n",
              "      <td>0.3064</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.065217</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>[he, was, almost, choking, there, was, so, muc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>“Your sister asked for it, I suppose?”</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4.571429</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>[your, sister, asked, for, it, i, suppose]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>She was engaged one day as she walked, in per...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.00312</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.001182</td>\n",
              "      <td>57</td>\n",
              "      <td>50</td>\n",
              "      <td>320</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>4.614035</td>\n",
              "      <td>4</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.280702</td>\n",
              "      <td>[she, was, engaged, one, day, as, she, walked,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The captain was in the porch, keeping himself ...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>58</td>\n",
              "      <td>49</td>\n",
              "      <td>319</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>4.517241</td>\n",
              "      <td>7</td>\n",
              "      <td>0.6908</td>\n",
              "      <td>0.293103</td>\n",
              "      <td>0.051724</td>\n",
              "      <td>0.189655</td>\n",
              "      <td>[the, captain, was, in, the, porch, keeping, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.00156</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>39</td>\n",
              "      <td>36</td>\n",
              "      <td>228</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>4.871795</td>\n",
              "      <td>4</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.025641</td>\n",
              "      <td>0.205128</td>\n",
              "      <td>[have, mercy, gentlemen, odin, flung, up, his,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                              words\n",
              "0      0  ...  [he, was, almost, choking, there, was, so, muc...\n",
              "1      1  ...         [your, sister, asked, for, it, i, suppose]\n",
              "2      2  ...  [she, was, engaged, one, day, as, she, walked,...\n",
              "3      3  ...  [the, captain, was, in, the, porch, keeping, h...\n",
              "4      4  ...  [have, mercy, gentlemen, odin, flung, up, his,...\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USTriTNSm6il",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "402bb7ff-5226-455c-c45e-29abbe4214f4"
      },
      "source": [
        "test.head(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>persons_0</th>\n",
              "      <th>persons_1</th>\n",
              "      <th>persons_2</th>\n",
              "      <th>persons_3</th>\n",
              "      <th>persons_4</th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_stopwords</th>\n",
              "      <th>num_words_upper</th>\n",
              "      <th>mean_word_len</th>\n",
              "      <th>num_words_title</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>noun</th>\n",
              "      <th>adj</th>\n",
              "      <th>verbs</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>“Not at all. I think she is one of the most ch...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>89</td>\n",
              "      <td>68</td>\n",
              "      <td>456</td>\n",
              "      <td>49</td>\n",
              "      <td>5</td>\n",
              "      <td>4.134831</td>\n",
              "      <td>9</td>\n",
              "      <td>0.9440</td>\n",
              "      <td>0.179775</td>\n",
              "      <td>0.123596</td>\n",
              "      <td>0.191011</td>\n",
              "      <td>[not, at, all, i, think, she, is, one, of, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43</td>\n",
              "      <td>36</td>\n",
              "      <td>221</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>4.162791</td>\n",
              "      <td>5</td>\n",
              "      <td>0.5739</td>\n",
              "      <td>0.139535</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.232558</td>\n",
              "      <td>[no, replied, he, with, sudden, consciousness,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>As the lady had stated her intention of scream...</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.00156</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>64</td>\n",
              "      <td>55</td>\n",
              "      <td>375</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>4.875000</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.7050</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>[as, the, lady, had, stated, her, intention, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>“And then suddenly in the silence I heard a so...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>240</td>\n",
              "      <td>150</td>\n",
              "      <td>1218</td>\n",
              "      <td>121</td>\n",
              "      <td>11</td>\n",
              "      <td>4.079167</td>\n",
              "      <td>21</td>\n",
              "      <td>-0.9787</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.041667</td>\n",
              "      <td>0.158333</td>\n",
              "      <td>[and, then, suddenly, in, the, silence, i, hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>His conviction remained unchanged. So far as I...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91</td>\n",
              "      <td>71</td>\n",
              "      <td>510</td>\n",
              "      <td>44</td>\n",
              "      <td>4</td>\n",
              "      <td>4.615385</td>\n",
              "      <td>8</td>\n",
              "      <td>0.9414</td>\n",
              "      <td>0.164835</td>\n",
              "      <td>0.098901</td>\n",
              "      <td>0.186813</td>\n",
              "      <td>[his, conviction, remained, unchanged, so, far...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                              words\n",
              "0      0  ...  [not, at, all, i, think, she, is, one, of, the...\n",
              "1      1  ...  [no, replied, he, with, sudden, consciousness,...\n",
              "2      2  ...  [as, the, lady, had, stated, her, intention, o...\n",
              "3      3  ...  [and, then, suddenly, in, the, silence, i, hea...\n",
              "4      4  ...  [his, conviction, remained, unchanged, so, far...\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dChLF1m6il"
      },
      "source": [
        "#import\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import naive_bayes, metrics, model_selection\n",
        "\n",
        "#모델들 \n",
        "\n",
        "def runLR(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=LogisticRegression(max_iter=1000)\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "def runSGD(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=SGDClassifier(loss='log')\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "def runRF(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=RandomForestClassifier()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "def runMLP(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=MLPClassifier()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "def runDT(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=DecisionTreeClassifier()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model\n",
        "\n",
        "def runMNB(train_X,train_y,test_X,test_y,test_X2):\n",
        "    model=naive_bayes.MultinomialNB()\n",
        "    model.fit(train_X,train_y)\n",
        "    pred_test_y=model.predict_proba(test_X)\n",
        "    pred_test_y2=model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU5Q8dLqm6im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c46967-97b9-4be9-9414-9a011ed998af"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Tfidf + LR\n",
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=50)\n",
        "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
        "train_y = train['author']\n",
        "\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "\n",
        "for dev_index, val_index in tqdm(cv.split(train_X,train_y)):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_LR_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_LR_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_LR_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_LR_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_LR_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_LR_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_LR_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_LR_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_LR_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_LR_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n",
            "5it [01:55, 23.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean cv score :  0.685600629942216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUoxb4W-m6im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1feb83b-aa72-4afb-fbbb-66385fbee4c7"
      },
      "source": [
        "#Tfidf + SGD\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runSGD(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_SGD_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_SGD_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_SGD_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_SGD_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_SGD_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_SGD_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_SGD_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_SGD_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_SGD_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_SGD_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean cv score :  0.8855549507495477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUVnGeccm6in",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be92159-9e89-4eb1-f8ae-f43c8e598c70"
      },
      "source": [
        "#Tfidf + RF\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runRF(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_RF_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_RF_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_RF_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_RF_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_RF_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_RF_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_RF_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_RF_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_RF_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_RF_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean cv score :  0.9601250937183746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aGB8TVbm6in",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d185f61-2efa-4d95-f308-699ee9dff67f"
      },
      "source": [
        "#Tfidf+MLP\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "\n",
        "for dev_index, val_index in tqdm(cv.split(train_X,train_y)):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMLP(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_MLP_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_MLP_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_MLP_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_MLP_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_MLP_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_MLP_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_MLP_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_MLP_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_MLP_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_MLP_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "1it [07:16, 436.40s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FppLQRTjm6in"
      },
      "source": [
        "#Tfidf+Decision Tree\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "\n",
        "\n",
        "for dev_index, val_index in tqdm(cv.split(train_X,train_y)):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runDT(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"tfidf_DT_0\"] = pred_train[:,0]\n",
        "train[\"tfidf_DT_1\"] = pred_train[:,1]\n",
        "train[\"tfidf_DT_2\"] = pred_train[:,2]\n",
        "train[\"tfidf_DT_3\"] = pred_train[:,3]\n",
        "train[\"tfidf_DT_4\"] = pred_train[:,4]\n",
        "test[\"tfidf_DT_0\"] = pred_full_test[:,0]\n",
        "test[\"tfidf_DT_1\"] = pred_full_test[:,1]\n",
        "test[\"tfidf_DT_2\"] = pred_full_test[:,2]\n",
        "test[\"tfidf_DT_3\"] = pred_full_test[:,3]\n",
        "test[\"tfidf_DT_4\"] = pred_full_test[:,4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCSXThsGm6in"
      },
      "source": [
        "# ngram=2\n",
        "cvec_vec=CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=50)\n",
        "cvec_vec.fit(train['text'].values.tolist())\n",
        "train_cvec = cvec_vec.transform(train['text'].values.tolist())\n",
        "test_cvec = cvec_vec.transform(test['text'].values.tolist())\n",
        "\n",
        "# CountV + LR\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "\n",
        "for dev_index, val_index in tqdm(cv.split(train_X,train_y)):\n",
        "    dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "train[\"cvec_LR_0\"] = pred_train[:,0]\n",
        "train[\"cvec_LR_1\"] = pred_train[:,1]\n",
        "train[\"cvec_LR_2\"] = pred_train[:,2]\n",
        "train[\"cvec_LR_3\"] = pred_train[:,3]\n",
        "train[\"cvec_LR_4\"] = pred_train[:,4]\n",
        "test[\"cvec_LR_0\"] = pred_full_test[:,0]\n",
        "test[\"cvec_LR_1\"] = pred_full_test[:,1]\n",
        "test[\"cvec_LR_2\"] = pred_full_test[:,2]\n",
        "test[\"cvec_LR_3\"] = pred_full_test[:,3]\n",
        "test[\"cvec_LR_4\"] = pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN7qIARRdXur"
      },
      "source": [
        "!pip install textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjnWB_DldYMb"
      },
      "source": [
        "from textstat import flesch_reading_ease\r\n",
        "\r\n",
        "train['mean_word_len']=train['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\r\n",
        "test['mean_word_len']=test['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\r\n",
        "train[\"chars_between_comma\"] = train[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/train[\"num_chars\"]\r\n",
        "test[\"chars_between_comma\"] = test[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/test[\"num_chars\"]\r\n",
        "train['ease']=train['text'].apply(flesch_reading_ease)\r\n",
        "test['ease']=test['text'].apply(flesch_reading_ease)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vULmHrOddesd"
      },
      "source": [
        "stop_trn = [x for x in train['words']]\r\n",
        "train['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/(len(sentence)+1) for sentence in stop_trn]\r\n",
        "print('process completed ')\r\n",
        "\r\n",
        "stop_tst = [x for x in test['words']]\r\n",
        "test['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/(len(sentence)+1) for sentence in stop_tst] \r\n",
        "print('process completed ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnxH16xUdhiO"
      },
      "source": [
        "def extract_features(df):\r\n",
        "    df['len'] = df['text'].apply(lambda x: len(x))\r\n",
        "    df['n_words'] = df['text'].apply(lambda x: len(x.split(' ')))\r\n",
        "    df['n_.'] = df['text'].str.count('\\.')\r\n",
        "    df['n_...'] = df['text'].str.count('\\...')\r\n",
        "    df['n_,'] = df['text'].str.count('\\,')\r\n",
        "    df['n_:'] = df['text'].str.count('\\:')\r\n",
        "    df['n_;'] = df['text'].str.count('\\;')\r\n",
        "    df['n_-'] = df['text'].str.count('\\-')\r\n",
        "    df['n_?'] = df['text'].str.count('\\?')\r\n",
        "    df['n_!'] = df['text'].str.count('\\!')\r\n",
        "    df['n_\\''] = df['text'].str.count('\\'')\r\n",
        "    df['n_\"'] = df['text'].str.count('\\\"')\r\n",
        "\r\n",
        "    # First words in a sentence\r\n",
        "    df['n_The '] = df['text'].str.count('The ')\r\n",
        "    df['n_I '] = df['text'].str.count('I ')\r\n",
        "    df['n_It '] = df['text'].str.count('It ')\r\n",
        "    df['n_He '] = df['text'].str.count('He ')\r\n",
        "    df['n_Me '] = df['text'].str.count('Me ')\r\n",
        "    df['n_She '] = df['text'].str.count('She ')\r\n",
        "    df['n_We '] = df['text'].str.count('We ')\r\n",
        "    df['n_They '] = df['text'].str.count('They ')\r\n",
        "    df['n_You '] = df['text'].str.count('You ')\r\n",
        "    df['n_the'] = df['words'].str.count('the ')\r\n",
        "    df['n_ a '] = df['words'].str.count(' a ')\r\n",
        "    df['n_appear'] = df['words'].str.count('appear')\r\n",
        "    df['n_little'] = df['words'].str.count('little')\r\n",
        "    df['n_was '] = df['words'].str.count('was ')\r\n",
        "    df['n_one '] = df['words'].str.count('one ')\r\n",
        "    df['n_two '] = df['words'].str.count('two ')\r\n",
        "    df['n_three '] = df['words'].str.count('three ')\r\n",
        "    df['n_ten '] = df['words'].str.count('ten ')\r\n",
        "    df['n_is '] = df['words'].str.count('is ')\r\n",
        "    df['n_are '] = df['words'].str.count('are ')\r\n",
        "    df['n_ed'] = df['words'].str.count('ed ')\r\n",
        "    df['n_however'] = df['words'].str.count('however')\r\n",
        "    df['n_ to '] = df['words'].str.count(' to ')\r\n",
        "    df['n_into'] = df['words'].str.count('into')\r\n",
        "    df['n_about '] = df['words'].str.count('about ')\r\n",
        "    df['n_th'] = df['words'].str.count('th')\r\n",
        "    df['n_er'] = df['words'].str.count('er')\r\n",
        "    df['n_ex'] = df['words'].str.count('ex')\r\n",
        "    df['n_an '] = df['words'].str.count('an ')\r\n",
        "    df['n_ground'] = df['words'].str.count('ground')\r\n",
        "    df['n_any'] = df['words'].str.count('any')\r\n",
        "    df['n_silence'] = df['words'].str.count('silence')\r\n",
        "    df['n_wall'] = df['words'].str.count('wall')\r\n",
        "\r\n",
        "    df.drop(['words'], axis=1, inplace=True)\r\n",
        "    \r\n",
        "print('Processing train...')\r\n",
        "extract_features(train)\r\n",
        "print('Processing test...')\r\n",
        "extract_features(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTp2HsrSeGzb"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=50)\r\n",
        "svd_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\r\n",
        "train_tfidf = tfidf_vec.transform(train['text'].values.tolist())\r\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\r\n",
        "train_y = train['author']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgy4_4XVleIW"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=50)\r\n",
        "svd_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\r\n",
        "train_tfidf = tfidf_vec.transform(train['text'].values.tolist())\r\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\r\n",
        "train_y = train['author']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsscrYyLm6io"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaP4KJxnm6io"
      },
      "source": [
        "#Tfidf + MNB\n",
        "cv_scores=[]\n",
        "cols_to_drop=['text','index']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "pred_train=np.zeros([train.shape[0],5])\n",
        "pred_full_test = 0\n",
        "\n",
        "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "\n",
        "for dev_index, val_index in cv.split(train_X,train_y):\n",
        "    dev_X, val_X = svd_tfidf[dev_index], svd_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "X_train[\"tfidf_words_nb_cvec_0\"] = pred_train[:,0]\n",
        "X_train[\"tfidf_words_nb_cvec_1\"] = pred_train[:,1]\n",
        "X_train[\"tfidf_words_nb_cvec_2\"] = pred_train[:,2]\n",
        "X_train[\"tfidf_words_nb_cvec_3\"] = pred_train[:,3]\n",
        "X_train[\"tfidf_words_nb_cvec_4\"] = pred_train[:,4]\n",
        "X_test[\"tfidf_words_nb_cvec_0\"] =  pred_full_test[:,0]\n",
        "X_test[\"tfidf_words_nb_cvec_1\"] =  pred_full_test[:,1]\n",
        "X_test[\"tfidf_words_nb_cvec_2\"] =  pred_full_test[:,2]\n",
        "X_test[\"tfidf_words_nb_cvec_3\"] =  pred_full_test[:,3]\n",
        "X_test[\"tfidf_words_nb_cvec_4\"] =  pred_full_test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMHy_DmAm6io"
      },
      "source": [
        "n_comp = 5\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(train_tfidf)\n",
        "\n",
        "train_svd = svd_obj.transform(train_tfidf)\n",
        "test_svd = svd_obj.transform(test_tfidf)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(train_svd)\n",
        "train_svd_scl = pd.DataFrame(scl.transform(train_svd))\n",
        "test_svd_scl = pd.DataFrame(scl.transform(test_svd))\n",
        "\n",
        "train_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "test_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "train = pd.concat([train, train_svd_scl], axis=1)\n",
        "test = pd.concat([test, test_svd_scl], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O-1hZNjm6ip"
      },
      "source": [
        "from sklearn import model_selection\n",
        "import xgboost as xgb\n",
        "\n",
        "cols_to_drop = ['index', 'text']\n",
        "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
        "train_y=train['author']\n",
        "test_index = test['index'].values\n",
        "test_X = test.drop(cols_to_drop, axis=1)\n",
        "xgb_preds=[]\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    \n",
        "    dtrain = xgb.DMatrix(dev_X,label=dev_y)\n",
        "    dvalid = xgb.DMatrix(val_X, label=val_y)\n",
        "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "\n",
        "    param = {}\n",
        "    param['objective'] = 'multi:softprob'\n",
        "    param['eta'] = 0.1\n",
        "    param['max_depth'] = 3\n",
        "    param['silent'] = 1\n",
        "    param['num_class'] = 5\n",
        "    param['eval_metric'] = \"mlogloss\"\n",
        "    param['min_child_weight'] = 1\n",
        "    param['subsample'] = 0.8\n",
        "    param['colsample_bytree'] = 0.3\n",
        "    param['seed'] = 0\n",
        "    param['tree_method'] = 'gpu_hist'\n",
        "\n",
        "    model = xgb.train(param, dtrain, 2000, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
        "\n",
        "    xgtest2 = xgb.DMatrix(test_X)\n",
        "    xgb_pred = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
        "    xgb_preds.append(list(xgb_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeDNq8zqm6ip"
      },
      "source": [
        "for i in range(len(xgb_preds[0])):\r\n",
        "    sum=0\r\n",
        "    for j in range(5):\r\n",
        "        sum+=xgb_preds[j][i]    \r\n",
        "    if(i==0):\r\n",
        "        preds=sum/5\r\n",
        "    else:\r\n",
        "        preds=np.vstack([preds,sum/5])\r\n",
        "\r\n",
        "preds=pd.DataFrame(preds)\r\n",
        "\r\n",
        "preds.to_csv('new_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}