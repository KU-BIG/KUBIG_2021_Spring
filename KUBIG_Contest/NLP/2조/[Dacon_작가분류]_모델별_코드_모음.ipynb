{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Dacon_작가분류] 모델별 코드 모음.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "V4WyG6cS0is6",
        "NPLc_hJ2zLNf",
        "ckkV81kOzLNg",
        "BmCdO5w-zLNj",
        "5AuOykJRX8gK",
        "7hrYGtEWYnoY",
        "_GdsxBKvzoez",
        "WKAGVz0WzYjq",
        "7rMZ8uRZztQj",
        "iTyiBTsy08Ie",
        "ev9eefLl1FIq",
        "D6WtsqV10wXW",
        "fClTvSoS6mGI",
        "LZFAthxLoeGD",
        "ipTLcLjX_Flr",
        "qBGMiImGxXH7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huHaVWj1O6kp"
      },
      "source": [
        "# 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2GgRBf2O2lU",
        "outputId": "01c56b39-a75d-4ddb-d0bf-b9a426d6829c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive/')\r\n",
        "from IPython.display import display\r\n",
        "import IPython\r\n",
        "import numpy as np\r\n",
        "import os,sys\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import scipy as sp\r\n",
        "import sklearn as skl\r\n",
        "%matplotlib notebook\r\n",
        "%matplotlib inline\r\n",
        "import tensorflow as tf\r\n",
        "!pip install mglearn\r\n",
        "!pip install gensim\r\n",
        "!pip install nltk\r\n",
        "import nltk\r\n",
        "import mglearn as mglearn\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import datasets, layers, models\r\n",
        "import re\r\n",
        "import urllib.request\r\n",
        "!pip install konlpy\r\n",
        "from konlpy.tag import Okt\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "!pip install torchtext --upgrade\r\n",
        "#https://github.com/pytorch/text/releases\r\n",
        "import torchtext\r\n",
        "torch.manual_seed(1)\r\n",
        "\r\n",
        "from time import time  # To time our operations\r\n",
        "from collections import defaultdict  # For word frequency\r\n",
        "\r\n",
        "import spacy  # For preprocessing\r\n",
        "\r\n",
        "import logging  # Setting up the loggings to monitor gensim\r\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\r\n",
        "\r\n",
        "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\r\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\r\n",
        "print(\"다음 기기로 학습합니다:\", device)\r\n",
        "os.chdir(\"/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/\")\r\n",
        "print(f\"현재 경로 : {os.getcwd()}\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive/\n",
            "Collecting mglearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/38/8aced26fce0b2ae82c3c87cd3b6105f38ca6d9d51704ecc44aa54473e6b9/mglearn-0.1.9.tar.gz (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mglearn) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from mglearn) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.1.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from mglearn) (7.0.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.7/dist-packages (from mglearn) (0.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from mglearn) (2.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mglearn) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mglearn) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mglearn) (2.8.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->mglearn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->mglearn) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler->mglearn) (1.15.0)\n",
            "Building wheels for collected packages: mglearn\n",
            "  Building wheel for mglearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582638 sha256=69782908d0b0ee817b067f5acdda9c9087b3b3bcad67f7d3082570aad33c0b3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/a6/ea/a6a3716233fa62fc561259b5cb1e28f79e9ff3592c0adac5f0\n",
            "Successfully built mglearn\n",
            "Installing collected packages: mglearn\n",
            "Successfully installed mglearn-0.1.9\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.8.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext) (3.7.4.3)\n",
            "다음 기기로 학습합니다: cuda\n",
            "현재 경로 : /gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4WyG6cS0is6"
      },
      "source": [
        "# 서브 함수들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94MhvbF0mW6"
      },
      "source": [
        "import json\r\n",
        "from collections import OrderedDict\r\n",
        "def ModelPath(model_name):\r\n",
        "  Basic = \"/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/Models/\"\r\n",
        "  return Basic+model_name+'.pt'\r\n",
        "\r\n",
        "def JsonPath(model_name):\r\n",
        "  Basic = \"/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/Models/\" \r\n",
        "  return Basic+model_name+'.json'\r\n",
        "\r\n",
        "def Directory():\r\n",
        "  while True:\r\n",
        "    try:\r\n",
        "      directory=f\"/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/Models/\"\r\n",
        "      if not os.path.exists(directory):\r\n",
        "            os.makedirs(directory)\r\n",
        "            os.chdir(directory)\r\n",
        "            return directory\r\n",
        "            break\r\n",
        "      else:\r\n",
        "        os.chdir(directory)\r\n",
        "        return directory\r\n",
        "        break\r\n",
        "    except OSError:\r\n",
        "      print ('Error: Creating directory. ' +  directory)\r\n",
        "      continue\r\n",
        "\r\n",
        "def SaveModels(model_name):\r\n",
        "  import json\r\n",
        "  from collections import OrderedDict\r\n",
        "  PATH = Directory()+model_name\r\n",
        "  \r\n",
        "  torch.save({\r\n",
        "            'model_state_dict': model_ft.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer_ft.state_dict(),\r\n",
        "             }, PATH)\r\n",
        "  print(f'Saved Path :{PATH}')\r\n",
        "\r\n",
        "def LoadModels(PATH,model,optimizer):\r\n",
        "  checkpoint = torch.load(PATH)\r\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "  model.train()\r\n",
        "  return model,optimizer\r\n",
        "\r\n",
        "def SaveJson(json_name,valid_loss,valid_acc):\r\n",
        "  import json\r\n",
        "  from collections import OrderedDict\r\n",
        "  PATH = json_name\r\n",
        "  model_info = OrderedDict()\r\n",
        "  model_info['name'] = model_name\r\n",
        "  model_info['path'] = PATH\r\n",
        "  model_info['Best Loss'] = str(min(valid_loss))\r\n",
        "  model_info['Best Acc'] = str(max(valid_acc).item())\r\n",
        "\r\n",
        "  with open(PATH,'w',encoding='utf-8') as make_file:\r\n",
        "    json.dump(model_info, make_file, ensure_ascii=False, indent='\\t')\r\n",
        "  print('Saving JSON completed!')\r\n",
        "\r\n",
        "def LoadJson(PATH):\r\n",
        "  with open(PATH,'r') as f:\r\n",
        "    model_info = json.load(f)\r\n",
        "  #print(json.dumps(model_info))\r\n",
        "\r\n",
        "  return model_info\r\n",
        "\r\n",
        "def today():\r\n",
        "  from datetime import datetime\r\n",
        "  year = str(datetime.today().year)\r\n",
        "\r\n",
        "  if datetime.today().month<10:\r\n",
        "    month = str(0)+ str(datetime.today().month)\r\n",
        "  else : month = str(datetime.today().month)\r\n",
        "\r\n",
        "  if datetime.today().day<10:\r\n",
        "    day = str(0)+ str(datetime.today().day)\r\n",
        "  else : day = str(datetime.today().day)\r\n",
        "\r\n",
        "  if (datetime.today().hour + 9)%24 < 10:\r\n",
        "    hour = str(0)+ str((datetime.today().hour+9)%24)\r\n",
        "  else : hour = str((datetime.today().hour+9)%24)\r\n",
        "\r\n",
        "  if datetime.today().minute<10:\r\n",
        "    min = str(0)+ str(datetime.today().minute)\r\n",
        "  else : min = str(datetime.today().minute)\r\n",
        "\r\n",
        "\r\n",
        "  return int(year+month+day)\r\n",
        "\r\n",
        "def JsonCheck():\r\n",
        "  PATH=\"/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/Models/\"\r\n",
        "  file_list=os.listdir(PATH)\r\n",
        "  json_list=[file for file in file_list if '.json' in file]\r\n",
        "  columns=['name', 'path', 'Best Loss', 'Best Acc']\r\n",
        "  check={}\r\n",
        "  for column in columns:\r\n",
        "    check[column]=[]\r\n",
        "  \r\n",
        "  for JSON in json_list:\r\n",
        "    info=LoadJson(PATH+JSON)\r\n",
        "    for key in columns:\r\n",
        "      if key in ['Best Loss', 'Best Acc']:\r\n",
        "        info[key]=round(float(info[key]),4)\r\n",
        "      check[key].append(info[key])\r\n",
        "  check_df=pd.DataFrame(check)\r\n",
        "  check_df=check_df[['name','Best Loss']]\r\n",
        "  check_df=check_df.sort_values(by='Best Loss')\r\n",
        "  return check_df\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRARvfwPndg"
      },
      "source": [
        "# 데이터 불러오기 read_csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdrk0mlCO70W",
        "outputId": "2a040991-4ef5-4761-fde7-4026d552a760"
      },
      "source": [
        "train=pd.read_csv('/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/train.csv',header=0,encoding='utf-8',verbose=True)\r\n",
        "test = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/test_x.csv',header=0,encoding='utf-8',verbose=True)\r\n",
        "train['author'].value_counts(sort=False).plot(kind='bar')\r\n",
        "print(train.groupby('author').size().reset_index(name = 'count'))\r\n",
        "maxcount=max(train.groupby('author').size())\r\n",
        "class_weight=[round(maxcount/count,3) for count in train.groupby('author').size()]\r\n",
        "class_weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenization took: 277.31 ms\n",
            "Type conversion took: 89.85 ms\n",
            "Parser memory cleanup took: 0.01 ms\n",
            "Tokenization took: 113.15 ms\n",
            "Type conversion took: 51.65 ms\n",
            "Parser memory cleanup took: 0.01 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPLc_hJ2zLNf"
      },
      "source": [
        "# EDA\r\n",
        "[텍스트 분류 문제]\r\n",
        "1. 데이터 크기\r\n",
        "2. 데이터의 개수\r\n",
        "3. 각 리뷰의 문자 길이 분포\r\n",
        "4. 많이 사용된 단어\r\n",
        "5. 긍정, 부정 데이터의 분포\r\n",
        "6. 각 리뷰의 단어 개수 분포 : 띄어쓰기 기준\r\n",
        "7. 특수문자 및 대문자, 소문자, 숫자 비율"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckkV81kOzLNg"
      },
      "source": [
        "## 클래스 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "8d9m9LMSzLNh",
        "outputId": "0ceeba4c-8905-490b-f968-396c90e731ef"
      },
      "source": [
        "train['author'].value_counts(sort=False).plot(kind='bar')\r\n",
        "print(train.groupby('author').size().reset_index(name = 'count'))\r\n",
        "maxcount=max(train.groupby('author').size())\r\n",
        "class_weight=[round(maxcount/count,3) for count in train.groupby('author').size()]\r\n",
        "class_weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   author  count\n",
            "0       0  13235\n",
            "1       1   7222\n",
            "2       2  11554\n",
            "3       3  15063\n",
            "4       4   7805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.138, 2.086, 1.304, 1.0, 1.93]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD1CAYAAACyaJl6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASGUlEQVR4nO3df5DcdX3H8eerpPijrSbIldIk9DI11UHbKt4AHWecVtoQxDH8oQ5Mx6Q2NX8Yq7ZONbR/ZEbLDE47pTKttKmkBscSKbVDRtA0g1inU0GOH4KAlBPBXIYfVxOhLVYbffeP/cSs511yt3vZPbjnY2Znv9/35/Pdfe8O5LXfH3ubqkKStLT9xLAbkCQNn2EgSTIMJEmGgSQJw0CShGEgSQKWDbuBXp166qk1Ojo67DYk6Vnljjvu+M+qGplef9aGwejoKOPj48NuQ5KeVZI8OlPdw0SSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CSxLP4S2eSTqzRbTcOuwUAHrn8wmG3sCS4ZyBJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRJzCIMkO5M8meSrM4y9L0klObWtJ8mVSSaS3JPkrK65m5I81G6buuqvSXJv2+bKJFmoFydJmpu57Bl8HFg/vZhkNbAO+GZX+QJgbbttAa5qc08BtgPnAGcD25OsaNtcBbyja7sfey5J0ol13DCoqi8CB2cYugJ4P1BdtQ3ANdVxK7A8yenA+cC+qjpYVYeAfcD6Nvaiqrq1qgq4Briov5ckSZqvns4ZJNkAHKiqr0wbWgns71qfbLVj1SdnqEuSBmjef8I6yQuBP6ZziGigkmyhc/iJM844Y9BPL0nPWb3sGfwisAb4SpJHgFXAnUl+DjgArO6au6rVjlVfNUN9RlW1o6rGqmpsZGSkh9YlSTOZdxhU1b1V9bNVNVpVo3QO7ZxVVY8De4CN7aqic4GnquoxYC+wLsmKduJ4HbC3jT2d5Nx2FdFG4IYFem2SpDmay6Wl1wJfAl6WZDLJ5mNMvwl4GJgA/g54J0BVHQQ+BNzebh9sNdqcj7Vtvg58treXIknq1XHPGVTVJccZH+1aLmDrLPN2AjtnqI8DrzxeH5KkE8dvIEuSDANJkmEgScIwkCRhGEiS6OEbyM8Vo9tuHHYLADxy+YXDbkGS3DOQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJLEHMIgyc4kTyb5alftz5J8Lck9Sf45yfKusUuTTCR5MMn5XfX1rTaRZFtXfU2S21r9U0lOXsgXKEk6vrnsGXwcWD+ttg94ZVX9CvAfwKUASc4ELgZe0bb5aJKTkpwE/DVwAXAmcEmbC/Bh4IqqeilwCNjc1yuSJM3bccOgqr4IHJxW+5eqOtxWbwVWteUNwO6q+m5VfQOYAM5ut4mqeriqvgfsBjYkCfB64Pq2/S7goj5fkyRpnhbinMHvAp9tyyuB/V1jk602W/0lwLe7guVIfUZJtiQZTzI+NTW1AK1LkqDPMEjyJ8Bh4JML086xVdWOqhqrqrGRkZFBPKUkLQk9/+xlkt8B3gicV1XVygeA1V3TVrUas9S/BSxPsqztHXTPlyQNSE97BknWA+8H3lRVz3QN7QEuTvK8JGuAtcCXgduBte3KoZPpnGTe00LkFuDNbftNwA29vRRJUq/mcmnptcCXgJclmUyyGfgr4GeAfUnuTvI3AFV1H3AdcD/wOWBrVX2/fep/F7AXeAC4rs0F+ADwh0km6JxDuHpBX6Ek6biOe5ioqi6ZoTzrP9hVdRlw2Qz1m4CbZqg/TOdqI0nSkPgNZElS7yeQpeei0W03DrsFAB65/MJht6Alxj0DSZJhIEkyDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIElibr+BvDPJk0m+2lU7Jcm+JA+1+xWtniRXJplIck+Ss7q22dTmP5RkU1f9NUnubdtcmSQL/SIlScc2lz2DjwPrp9W2ATdX1Vrg5rYOcAGwtt22AFdBJzyA7cA5dH7vePuRAGlz3tG13fTnkiSdYMcNg6r6InBwWnkDsKst7wIu6qpfUx23AsuTnA6cD+yrqoNVdQjYB6xvYy+qqlurqoBruh5LkjQgvZ4zOK2qHmvLjwOnteWVwP6ueZOtdqz65Ax1SdIA9X0CuX2irwXo5biSbEkynmR8ampqEE8pSUtCr2HwRDvEQ7t/stUPAKu75q1qtWPVV81Qn1FV7aiqsaoaGxkZ6bF1SdJ0vYbBHuDIFUGbgBu66hvbVUXnAk+1w0l7gXVJVrQTx+uAvW3s6STntquINnY9liRpQJYdb0KSa4FfB05NMknnqqDLgeuSbAYeBd7apt8EvAGYAJ4B3g5QVQeTfAi4vc37YFUdOSn9TjpXLL0A+Gy7SZIG6LhhUFWXzDJ03gxzC9g6y+PsBHbOUB8HXnm8PiRJJ47fQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSRJ9hkOQPktyX5KtJrk3y/CRrktyWZCLJp5Kc3OY+r61PtPHRrse5tNUfTHJ+fy9JkjRfPYdBkpXAu4GxqnolcBJwMfBh4IqqeilwCNjcNtkMHGr1K9o8kpzZtnsFsB74aJKTeu1LkjR//R4mWga8IMky4IXAY8Drgevb+C7gora8oa3Txs9LklbfXVXfrapvABPA2X32JUmah57DoKoOAH8OfJNOCDwF3AF8u6oOt2mTwMq2vBLY37Y93Oa/pLs+wzY/IsmWJONJxqempnptXZI0TT+HiVbQ+VS/Bvh54KfoHOY5YapqR1WNVdXYyMjIiXwqSVpS+jlM9JvAN6pqqqr+D/g08FpgeTtsBLAKONCWDwCrAdr4i4Fvdddn2EaSNADLjj9lVt8Ezk3yQuA7wHnAOHAL8GZgN7AJuKHN39PWv9TGP19VlWQP8A9J/oLOHsZa4Mt99CVJC2p0243DbgGARy6/8IQ9ds9hUFW3JbkeuBM4DNwF7ABuBHYn+dNWu7ptcjXwiSQTwEE6VxBRVfcluQ64vz3O1qr6fq99SZLmr589A6pqO7B9WvlhZrgaqKr+F3jLLI9zGXBZP71IknrnN5AlSYaBJMkwkCRhGEiSMAwkSfR5NZGeG5bCNdSSjs09A0mSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk+gyDJMuTXJ/ka0keSPJrSU5Jsi/JQ+1+RZubJFcmmUhyT5Kzuh5nU5v/UJJN/b4oSdL89Ltn8BHgc1X1cuBXgQeAbcDNVbUWuLmtA1wArG23LcBVAElOofM7yufQ+e3k7UcCRJI0GD2HQZIXA68Drgaoqu9V1beBDcCuNm0XcFFb3gBcUx23AsuTnA6cD+yrqoNVdQjYB6zvtS9J0vz1s2ewBpgC/j7JXUk+luSngNOq6rE253HgtLa8Etjftf1kq81WlyQNSD9hsAw4C7iqql4N/A9HDwkBUFUFVB/P8SOSbEkynmR8ampqoR5Wkpa8fsJgEpisqtva+vV0wuGJdviHdv9kGz8ArO7aflWrzVb/MVW1o6rGqmpsZGSkj9YlSd16DoOqehzYn+RlrXQecD+wBzhyRdAm4Ia2vAfY2K4qOhd4qh1O2gusS7KinThe12qSpAHp9zeQfx/4ZJKTgYeBt9MJmOuSbAYeBd7a5t4EvAGYAJ5pc6mqg0k+BNze5n2wqg722ZckaR76CoOquhsYm2HovBnmFrB1lsfZCezspxdJUu/8BrIkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJBYgDJKclOSuJJ9p62uS3JZkIsmnkpzc6s9r6xNtfLTrMS5t9QeTnN9vT5Kk+VmIPYP3AA90rX8YuKKqXgocAja3+mbgUKtf0eaR5EzgYuAVwHrgo0lOWoC+JElz1FcYJFkFXAh8rK0HeD1wfZuyC7ioLW9o67Tx89r8DcDuqvpuVX0DmADO7qcvSdL89Ltn8JfA+4EftPWXAN+uqsNtfRJY2ZZXAvsB2vhTbf4P6zNs8yOSbEkynmR8amqqz9YlSUf0HAZJ3gg8WVV3LGA/x1RVO6pqrKrGRkZGBvW0kvSct6yPbV8LvCnJG4DnAy8CPgIsT7KsffpfBRxo8w8Aq4HJJMuAFwPf6qof0b2NJGkAet4zqKpLq2pVVY3SOQH8+ar6beAW4M1t2ibghra8p63Txj9fVdXqF7erjdYAa4Ev99qXJGn++tkzmM0HgN1J/hS4C7i61a8GPpFkAjhIJ0CoqvuSXAfcDxwGtlbV909AX5KkWSxIGFTVF4AvtOWHmeFqoKr6X+Ats2x/GXDZQvQiSZo/v4EsSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEn2EQZLVSW5Jcn+S+5K8p9VPSbIvyUPtfkWrJ8mVSSaS3JPkrK7H2tTmP5RkU/8vS5I0H/3sGRwG3ldVZwLnAluTnAlsA26uqrXAzW0d4AJgbbttAa6CTngA24Fz6Px28vYjASJJGoyew6CqHquqO9vyfwEPACuBDcCuNm0XcFFb3gBcUx23AsuTnA6cD+yrqoNVdQjYB6zvtS9J0vwtyDmDJKPAq4HbgNOq6rE29DhwWlteCezv2myy1WarS5IGpO8wSPLTwD8B762qp7vHqqqA6vc5up5rS5LxJONTU1ML9bCStOT1FQZJfpJOEHyyqj7dyk+0wz+0+ydb/QCwumvzVa02W/3HVNWOqhqrqrGRkZF+WpckdennaqIAVwMPVNVfdA3tAY5cEbQJuKGrvrFdVXQu8FQ7nLQXWJdkRTtxvK7VJEkDsqyPbV8LvA24N8ndrfbHwOXAdUk2A48Cb21jNwFvACaAZ4C3A1TVwSQfAm5v8z5YVQf76EuSNE89h0FV/RuQWYbPm2F+AVtneaydwM5ee5Ek9cdvIEuSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJYhGFQZL1SR5MMpFk27D7kaSlZFGEQZKTgL8GLgDOBC5JcuZwu5KkpWNRhAFwNjBRVQ9X1feA3cCGIfckSUtGqmrYPZDkzcD6qvq9tv424Jyqete0eVuALW31ZcCDA230x50K/OeQe1gsfC+O8r04yvfiqMXyXvxCVY1MLy4bRie9qqodwI5h93FEkvGqGht2H4uB78VRvhdH+V4ctdjfi8VymOgAsLprfVWrSZIGYLGEwe3A2iRrkpwMXAzsGXJPkrRkLIrDRFV1OMm7gL3AScDOqrpvyG3NxaI5ZLUI+F4c5XtxlO/FUYv6vVgUJ5AlScO1WA4TSZKGyDCQJBkGkqRFcgL52SLJy+l8M3plKx0A9lTVA8PrSsPW/rtYCdxWVf/dVV9fVZ8bXmeDl+RsoKrq9vYnZdYDX6uqm4bc2lAluaaqNg67j2PxBPIcJfkAcAmdP5Ux2cqr6FwGu7uqLh9Wb4tJkrdX1d8Pu49BSfJuYCvwAPAq4D1VdUMbu7Oqzhpmf4OUZDudvy+2DNgHnAPcAvwWsLeqLhtiewOTZPpl8QF+A/g8QFW9aeBNzYFhMEdJ/gN4RVX937T6ycB9VbV2OJ0tLkm+WVVnDLuPQUlyL/BrVfXfSUaB64FPVNVHktxVVa8eaoMD1N6LVwHPAx4HVlXV00leQGev6VeG2uCAJLkTuB/4GFB0wuBaOh8cqap/HV53s/Mw0dz9APh54NFp9dPb2JKR5J7ZhoDTBtnLIvATRw4NVdUjSX4duD7JL9B5P5aSw1X1feCZJF+vqqcBquo7SZbS/yNjwHuAPwH+qKruTvKdxRoCRxgGc/de4OYkDwH7W+0M4KXAu2bd6rnpNOB84NC0eoB/H3w7Q/VEkldV1d0AbQ/hjcBO4JeH29rAfS/JC6vqGeA1R4pJXswS+sBUVT8Arkjyj+3+CZ4F/9Yu+gYXi6r6XJJfovPntrtPIN/ePg0tJZ8BfvrIP4Ddknxh8O0M1UbgcHehqg4DG5P87XBaGprXVdV34Yf/IB7xk8Cm4bQ0PFU1CbwlyYXA08Pu53g8ZyBJ8nsGkiTDQJKEYSBJwjCQJGEYSJKA/wcFDnQLeNM1nQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCdO5w-zLNj"
      },
      "source": [
        "## 공백 없음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v8ifLXmzLNj",
        "outputId": "9cfa73cc-5e4b-4d3c-e8e3-1510a0439bd0"
      },
      "source": [
        "null_check=train.isnull().values.any()\r\n",
        "print(null_check)\r\n",
        "if null_check:\r\n",
        "  print(train.isnull().sum())\r\n",
        "  print(train.loc[train.text.isnull()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AuOykJRX8gK"
      },
      "source": [
        "# 데이터셋 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC_YA4QnUGFj",
        "outputId": "75274b71-261c-4507-ebb4-725f70a80315"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "!pip install sacremoses\r\n",
        "!pip install revtok\r\n",
        "import revtok\r\n",
        "import sacremoses\r\n",
        "import nltk.tokenize\r\n",
        "from torchtext.data.utils import get_tokenizer\r\n",
        "from torchtext import data\r\n",
        "from nltk.corpus import stopwords  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.41.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=23bcd561ed91ce734a5a3546f77784bcebc0b764b0514f0a49b233a32d6874f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.43\n",
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hrYGtEWYnoY"
      },
      "source": [
        "# 전처리 함수 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GdsxBKvzoez"
      },
      "source": [
        "## 1) Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPHZfouizrrb"
      },
      "source": [
        "def cleaning(sentences):\r\n",
        "  cleaned_text = []\r\n",
        "  for words in sentences:\r\n",
        "    words = re.sub('[^0-9A-Za-z\"]',' ', words).lower()\r\n",
        "    words = re.sub(r' +', ' ', words)\r\n",
        "    words = re.sub(r'\\n', ' ', words)\r\n",
        "    words = \" \".join(words.split())\r\n",
        "    cleaned_text.append(words)\r\n",
        "  return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKAGVz0WzYjq"
      },
      "source": [
        "## 2) Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rMZ8uRZztQj"
      },
      "source": [
        "### (1) spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "77WM2_rGX-S5",
        "outputId": "f143312b-37dd-4e21-98d7-1c03b342f3e4"
      },
      "source": [
        "tokenizer = get_tokenizer(\"spacy\")\r\n",
        "#tokenizer = get_tokenizer('toktok')\r\n",
        "nltk.download('stopwords')\r\n",
        "Stopwords=stopwords.words('english')\r\n",
        "Stopwords.extend(['','the','a','an','\"'])\r\n",
        "\r\n",
        "#초기세팅 \r\n",
        "Text = torchtext.data.Field(preprocessing = cleaning, tokenize=tokenizer,batch_first=True,stop_words=Stopwords)#,fix_length=max_sequence_length)\r\n",
        "Label = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\r\n",
        "Index = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None) \r\n",
        "fields = [('index',Index),('text', Text),('author', Label)]\r\n",
        "\r\n",
        "dataset = torchtext.data.TabularDataset(\r\n",
        "  path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/train.csv', format='csv',\r\n",
        "  fields=fields,\r\n",
        "  skip_header=True\r\n",
        ")\r\n",
        "from torchtext.data import Iterator\r\n",
        "batch_size=256\r\n",
        "\r\n",
        "train_loader = Iterator(dataset=trainset, batch_size = batch_size)\r\n",
        "valid_loader = Iterator(dataset=validset, batch_size = batch_size)\r\n",
        "test_loader = Iterator(dataset=testset, batch_size = batch_size)\r\n",
        "data_loader={'train':train_loader,'valid':valid_loader,'test':test_loader}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4830f740a32a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#초기세팅\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,fix_length=max_sequence_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "4_OFAvzhbd_T",
        "outputId": "b782cd87-8857-4094-e2a8-e0ee5d3430a4"
      },
      "source": [
        "seq_length=[]\r\n",
        "for i in range(len(dataset)):\r\n",
        "  seq_length.append(len(vars(dataset[i])['text']))\r\n",
        "\r\n",
        "print(f\"평균 : {int(np.mean(seq_length))}\")\r\n",
        "print(f\"중앙값 : {np.median(seq_length)}\")\r\n",
        "print(f'최소값 : {np.min(seq_length)}')\r\n",
        "print(f'최대값 : {np.max(seq_length)}')\r\n",
        "for i,q in enumerate(range(20,110,10)): \r\n",
        "  print(f'{q}% 수 : {np.percentile(seq_length,q)}')\r\n",
        "print(f'95% 수 : {np.percentile(seq_length,95)}')\r\n",
        "print(f'99% 수 : {np.percentile(seq_length,99)}')\r\n",
        "print()\r\n",
        "plt.boxplot(seq_length,vert=False)\r\n",
        "plt.show()\r\n",
        "#pd.Series(seq_length).plot(kind='line')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "평균 : 31\n",
            "중앙값 : 19.0\n",
            "최소값 : 2\n",
            "최대값 : 296\n",
            "20% 수 : 10.0\n",
            "30% 수 : 13.0\n",
            "40% 수 : 16.0\n",
            "50% 수 : 19.0\n",
            "60% 수 : 23.0\n",
            "70% 수 : 31.0\n",
            "80% 수 : 46.0\n",
            "90% 수 : 76.0\n",
            "100% 수 : 296.0\n",
            "95% 수 : 106.0\n",
            "99% 수 : 163.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCbr4Emwdqj_"
      },
      "source": [
        "Fields 재 설정 후 데이터 셋 구성 : max_sequence_length = 106"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNr8NrhpdqIp"
      },
      "source": [
        "max_sequence_length = 106\r\n",
        "Text = torchtext.data.Field(preprocessing = cleaning, tokenize=tokenizer,batch_first=True,stop_words=Stopwords,fix_length=max_sequence_length)\r\n",
        "Label = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\r\n",
        "Index = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None) \r\n",
        "fields = [('index',Index),('text', Text),('author', Label)]\r\n",
        "test_fields=[('index',Index),('text', Text)]\r\n",
        "\r\n",
        "dataset = torchtext.data.TabularDataset(\r\n",
        "  path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/train.csv', format='csv',\r\n",
        "  fields=fields,\r\n",
        "  skip_header=True\r\n",
        ")\r\n",
        "\r\n",
        "testset = torchtext.data.TabularDataset(\r\n",
        "  path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/test_x.csv', format='csv',\r\n",
        "  fields=test_fields,\r\n",
        "  skip_header=True\r\n",
        ")\r\n",
        "\r\n",
        "Text.build_vocab(dataset,max_size=10000,min_freq=3) \r\n",
        "vocab = Text.vocab\r\n",
        "vocab.freqs\r\n",
        "vocab_size=len(vocab)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPQCSOxIYbhN"
      },
      "source": [
        "import random\r\n",
        "trainset,validset=dataset.split(split_ratio=0.7,strata_field='label',random_state=random.seed(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTyiBTsy08Ie"
      },
      "source": [
        "### (2) toktok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En-DjzAG08Ig",
        "outputId": "854a4705-7fa4-4e58-a93a-88f3f501cfa6"
      },
      "source": [
        "tokenizer = get_tokenizer('toktok')\r\n",
        "nltk.download('stopwords')\r\n",
        "Stopwords=stopwords.words('english')\r\n",
        "Stopwords.extend(['','the','a','an','\"'])\r\n",
        "\r\n",
        "#초기세팅 \r\n",
        "Text = torchtext.data.Field(preprocessing = cleaning, tokenize=tokenizer,batch_first=True,stop_words=Stopwords)#,fix_length=max_sequence_length)\r\n",
        "Label = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\r\n",
        "Index = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None) \r\n",
        "fields = [('index',Index),('text', Text),('author', Label)]\r\n",
        "\r\n",
        "dataset = torchtext.data.TabularDataset(\r\n",
        "  path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/train.csv', format='csv',\r\n",
        "  fields=fields,\r\n",
        "  skip_header=True\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "gw5xna9_08Ih",
        "outputId": "b782cd87-8857-4094-e2a8-e0ee5d3430a4"
      },
      "source": [
        "seq_length=[]\r\n",
        "for i in range(len(dataset)):\r\n",
        "  seq_length.append(len(vars(dataset[i])['text']))\r\n",
        "\r\n",
        "print(f\"평균 : {int(np.mean(seq_length))}\")\r\n",
        "print(f\"중앙값 : {np.median(seq_length)}\")\r\n",
        "print(f'최소값 : {np.min(seq_length)}')\r\n",
        "print(f'최대값 : {np.max(seq_length)}')\r\n",
        "for i,q in enumerate(range(20,110,10)): \r\n",
        "  print(f'{q}% 수 : {np.percentile(seq_length,q)}')\r\n",
        "print(f'95% 수 : {np.percentile(seq_length,95)}')\r\n",
        "print(f'99% 수 : {np.percentile(seq_length,99)}')\r\n",
        "print()\r\n",
        "plt.boxplot(seq_length,vert=False)\r\n",
        "plt.show()\r\n",
        "#pd.Series(seq_length).plot(kind='line')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "평균 : 31\n",
            "중앙값 : 19.0\n",
            "최소값 : 2\n",
            "최대값 : 296\n",
            "20% 수 : 10.0\n",
            "30% 수 : 13.0\n",
            "40% 수 : 16.0\n",
            "50% 수 : 19.0\n",
            "60% 수 : 23.0\n",
            "70% 수 : 31.0\n",
            "80% 수 : 46.0\n",
            "90% 수 : 76.0\n",
            "100% 수 : 296.0\n",
            "95% 수 : 106.0\n",
            "99% 수 : 163.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccJDWvnY08Ii"
      },
      "source": [
        "max_sequence_length = 106\r\n",
        "Text = torchtext.data.Field(preprocessing = cleaning, tokenize=tokenizer,batch_first=True,stop_words=Stopwords,fix_length=max_sequence_length)\r\n",
        "Label = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\r\n",
        "Index = torchtext.data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None) \r\n",
        "fields = [('index',Index),('text', Text),('author', Label)]\r\n",
        "test_fields=[('index',Index),('text', Text)]\r\n",
        "\r\n",
        "dataset = torchtext.data.TabularDataset(\r\n",
        "  path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/train.csv', format='csv',\r\n",
        "  fields=fields,\r\n",
        "  skip_header=True\r\n",
        ")\r\n",
        "\r\n",
        "testset = torchtext.data.TabularDataset(\r\n",
        "  path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/test_x.csv', format='csv',\r\n",
        "  fields=test_fields,\r\n",
        "  skip_header=True\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHLV3bxp08Ii"
      },
      "source": [
        "import random\r\n",
        "trainset,validset=dataset.split(split_ratio=0.7,strata_field='label',random_state=random.seed(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev9eefLl1FIq"
      },
      "source": [
        "## 3) 단어임베딩 : From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4izVWmazYxjk",
        "outputId": "4ce3c52b-6f97-4844-e266-3a38e3d02b92"
      },
      "source": [
        "Text.build_vocab(dataset,min_freq=5) \r\n",
        "vocab = Text.vocab\r\n",
        "vocab.freqs\r\n",
        "vocab_size=len(vocab)\r\n",
        "print('단어 집합의 크기 : {}'.format(len(Text.vocab)))\r\n",
        "list(Text.vocab.stoi.items())[:22],len(Text.vocab)\r\n",
        "#Text.vocab.itos[-20:],len(Text.vocab)\r\n",
        "#print(list(vocab.freqs.items())[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 14434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('<unk>', 0),\n",
              "  ('<pad>', 1),\n",
              "  ('', 2),\n",
              "  ('i', 3),\n",
              "  ('odin', 4),\n",
              "  ('s', 5),\n",
              "  ('said', 6),\n",
              "  ('one', 7),\n",
              "  ('the', 8),\n",
              "  ('would', 9),\n",
              "  ('he', 10),\n",
              "  ('mr', 11),\n",
              "  ('could', 12),\n",
              "  ('n t', 13),\n",
              "  ('it', 14),\n",
              "  ('man', 15),\n",
              "  ('you', 16),\n",
              "  ('upon', 17),\n",
              "  ('but', 18),\n",
              "  ('and', 19),\n",
              "  ('know', 20),\n",
              "  ('well', 21)],\n",
              " 14434)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "JYxkFPMVroBy",
        "outputId": "552a0acb-a7ec-405d-ac2a-46a498f65473"
      },
      "source": [
        "#단어 등장횟수 분석\r\n",
        "#여기에 임베딩을 활용할 수 있음\r\n",
        "\r\n",
        "frequencies =list(vocab.freqs.values())\r\n",
        "frequencies\r\n",
        "print(f\"평균 : {int(np.mean(frequencies))}\")\r\n",
        "print(f\"중앙값 : {np.median(frequencies)}\")\r\n",
        "print(f'최소값 : {np.min(frequencies)}')\r\n",
        "print(f'최대값 : {np.max(frequencies)}')\r\n",
        "for i,q in enumerate(range(25,100,25)): \r\n",
        "  print(f'{i+1} 분위 수 : {np.percentile(frequencies,q)}')\r\n",
        "print()\r\n",
        "plt.boxplot(frequencies,vert=False)\r\n",
        "plt.show()\r\n",
        "#pd.Series(seq_length).plot(kind='line')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "평균 : 47\n",
            "중앙값 : 3.0\n",
            "최소값 : 1\n",
            "최대값 : 500512\n",
            "1 분위 수 : 1.0\n",
            "2 분위 수 : 3.0\n",
            "3 분위 수 : 10.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RcvSk-EYDjV",
        "outputId": "13091095-2e93-442e-d8e0-ee8ffecf2f7f"
      },
      "source": [
        "#여기에 임베딩을 활용할 수 있음\r\n",
        "Text.build_vocab(dataset,max_size=10000,min_freq=3) \r\n",
        "\r\n",
        "vocab = Text.vocab\r\n",
        "vocab.freqs\r\n",
        "vocab_size=len(vocab)\r\n",
        "print('단어 집합의 크기 : {}'.format(len(Text.vocab)))\r\n",
        "#list(Text.vocab.stoi.items())[:22],len(Text.vocab)\r\n",
        "Text.vocab.itos[-20:],len(Text.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 10002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['lightness',\n",
              "  'limped',\n",
              "  'lobster',\n",
              "  'loch',\n",
              "  'locket',\n",
              "  'loosed',\n",
              "  'loyalty',\n",
              "  'lucases',\n",
              "  'ludwigovna',\n",
              "  'lumbering',\n",
              "  'magnitude',\n",
              "  'mahogany',\n",
              "  'malevolent',\n",
              "  'maliciously',\n",
              "  'managing',\n",
              "  'mangled',\n",
              "  'manhood',\n",
              "  'manse',\n",
              "  'marriages',\n",
              "  'martyrdom'],\n",
              " 10002)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6WtsqV10wXW"
      },
      "source": [
        "# 데이터 로더"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPZrs3Gf0XMF"
      },
      "source": [
        "from torchtext.data import Iterator\r\n",
        "batch_size=256\r\n",
        "\r\n",
        "train_loader = Iterator(dataset=trainset, batch_size = batch_size)\r\n",
        "valid_loader = Iterator(dataset=validset, batch_size = batch_size)\r\n",
        "test_loader = Iterator(dataset=testset, batch_size = batch_size)\r\n",
        "data_loader={'train':train_loader,'valid':valid_loader,'test':test_loader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJoUCMil2OSv",
        "outputId": "c0daebcb-def1-4146-eabe-48ef9001f57a"
      },
      "source": [
        "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\r\n",
        "print('검증용 데이터의 미니 배치 수 : {}'.format(len(valid_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 미니 배치 수 : 151\n",
            "검증용 데이터의 미니 배치 수 : 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pehokl8iyK_Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csB8sYox4qWr"
      },
      "source": [
        "# 토크나이저에 따른 통합함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA5b7ceD3dFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47aa7ed4-1c1e-473f-992c-ff3f2a531e8f"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "!pip install sacremoses\r\n",
        "!pip install revtok\r\n",
        "import revtok\r\n",
        "import sacremoses\r\n",
        "import nltk.tokenize\r\n",
        "import random\r\n",
        "from torchtext.data.utils import get_tokenizer\r\n",
        "from torchtext import data\r\n",
        "from nltk.corpus import stopwords  \r\n",
        "from torchtext.legacy.data import Field,TabularDataset,Iterator\r\n",
        "def cleaning(sentences):\r\n",
        "  cleaned_text = []\r\n",
        "  for words in sentences:\r\n",
        "    words = re.sub('[^0-9A-Za-z\"]',' ', words).lower()\r\n",
        "    words = re.sub(r' +', ' ', words)\r\n",
        "    words = re.sub(r'\\n', ' ', words)\r\n",
        "    words = \" \".join(words.split())\r\n",
        "    cleaned_text.append(words)\r\n",
        "  return cleaned_text\r\n",
        "  \r\n",
        "def MAX_SEQ_LEN(tok,n):\r\n",
        "  #초기세팅\r\n",
        "  tokenizer = get_tokenizer(tok)\r\n",
        "  nltk.download('stopwords')\r\n",
        "  Stopwords=stopwords.words('english')\r\n",
        "  Stopwords.extend(['','the','a','an','\"']) \r\n",
        "  \r\n",
        "  Text = Field(preprocessing = cleaning, tokenize=tokenizer,batch_first=True,stop_words=Stopwords)#,fix_length=max_sequence_length)\r\n",
        "  Label = Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\r\n",
        "  Index = Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None) \r\n",
        "  fields = [('index',Index),('text', Text),('author', Label)]\r\n",
        "\r\n",
        "  dataset = TabularDataset(path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/train.csv', format='csv',fields=fields,\r\n",
        "                                          skip_header=True)\r\n",
        "  seq_length=[]\r\n",
        "  for i in range(len(dataset)):\r\n",
        "    seq_length.append(len(vars(dataset[i])['text']))\r\n",
        "  return int(np.percentile(seq_length,n))\r\n",
        "\r\n",
        "def make_dataset_by_tokenizer(tok,n):\r\n",
        "  tokenizer = get_tokenizer(tok)\r\n",
        "  nltk.download('stopwords')\r\n",
        "  Stopwords=stopwords.words('english')\r\n",
        "  Stopwords.extend(['','the','a','an','\"'])\r\n",
        "  max_sequence_length = MAX_SEQ_LEN(tok,n)\r\n",
        "  Text = Field(preprocessing = cleaning, tokenize=tokenizer,batch_first=True,stop_words=Stopwords,fix_length=max_sequence_length)\r\n",
        "  Label = Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\r\n",
        "  Index = Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None) \r\n",
        "  fields = [('index',Index),('text', Text),('author', Label)]\r\n",
        "  test_fields=[('index',Index),('text', Text)]\r\n",
        "\r\n",
        "  dataset = TabularDataset(\r\n",
        "    path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/train.csv', format='csv',\r\n",
        "    fields=fields,\r\n",
        "    skip_header=True\r\n",
        "  )\r\n",
        "\r\n",
        "  testset = TabularDataset(\r\n",
        "    path='/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/test_x.csv', format='csv',\r\n",
        "    fields=test_fields,\r\n",
        "    skip_header=True\r\n",
        "  )\r\n",
        "  Text.build_vocab(dataset,min_freq=2) \r\n",
        "\r\n",
        "  vocab = Text.vocab\r\n",
        "  vocab.freqs\r\n",
        "  vocab_size=len(vocab)\r\n",
        "  \r\n",
        "\r\n",
        "  trainset,validset=dataset.split(split_ratio=0.8,strata_field='label',random_state=random.seed(3))\r\n",
        "\r\n",
        "  batch_size=256\r\n",
        "\r\n",
        "  train_loader = Iterator(dataset=trainset, batch_size = batch_size)\r\n",
        "  valid_loader = Iterator(dataset=validset, batch_size = batch_size)\r\n",
        "  test_loader = Iterator(dataset=testset, batch_size = batch_size,shuffle=False)\r\n",
        "  data_loader={'train':train_loader,'valid':valid_loader,'test':test_loader}\r\n",
        "  return data_loader,max_sequence_length,vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.41.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=81b048b860e225b963058e93672e9fdd810ce17808bcbbd27e17c69681a70fd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.43\n",
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y4vqswt9NQ_"
      },
      "source": [
        "# 학습함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlewf3Xy2Ryp"
      },
      "source": [
        "from torch.optim import lr_scheduler\r\n",
        "import copy\r\n",
        "import time\r\n",
        "import torchvision.models as models\r\n",
        "def train_model(model, criterion, optimizer, scheduler=None, num_epochs=1):\r\n",
        "    since = time.time()\r\n",
        "    max_grad_norm=5.\r\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "    best_acc = 0.0\r\n",
        "    best_loss = 100\r\n",
        "    train_acc=[]\r\n",
        "    valid_acc=[]\r\n",
        "    train_loss=[]\r\n",
        "    valid_loss=[]\r\n",
        "    \r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\r\n",
        "        print('-' * 10)\r\n",
        "\r\n",
        "        # Each epoch has a training and validationidation phase\r\n",
        "        for phase in ['train', 'valid']:\r\n",
        "            if phase == 'train':\r\n",
        "                model.train()  # Set model to training mode\r\n",
        "                \r\n",
        "            else:\r\n",
        "                model.eval()   # Set model to evalidationuate mode\r\n",
        "\r\n",
        "            running_loss = 0.0\r\n",
        "            running_corrects = 0\r\n",
        "            i=0\r\n",
        "            # Iterate over data.\r\n",
        "            for batch in data_loader[phase]:\r\n",
        "                text = batch.text.to(device)\r\n",
        "                batch.author=batch.author.to(device,dtype=torch.long)\r\n",
        "                #print(batch.label.shape)\r\n",
        "                # zero the parameter gradients\r\n",
        "                optimizer.zero_grad()\r\n",
        "                # forward\r\n",
        "                # track history if only in train\r\n",
        "                with torch.set_grad_enabled(phase == 'train'):\r\n",
        "                    predictions = model(text).to(device)             \r\n",
        "                    #print(predictions.shape)\r\n",
        "                    \r\n",
        "                    try :\r\n",
        "                      if model.lstm.num_layers >=1:\r\n",
        "                       #predictions = predictions.view(len(text),-1)#[:,-1]\r\n",
        "                       predictions = predictions.to(device)\r\n",
        "                                          \r\n",
        "                    except:\r\n",
        "                      predictions = predictions.to(device)\r\n",
        "                      #print(predictions.shape)\r\n",
        "                    \r\n",
        "                    preds=torch.argmax(predictions,dim=-1)\r\n",
        "                    #print(preds)\r\n",
        "             \r\n",
        "                    loss = criterion(predictions, batch.author)\r\n",
        "                    \r\n",
        "                    #print(loss)\r\n",
        "                    # backward + optimize only if in training phase\r\n",
        "                    if phase == 'train':\r\n",
        "                      loss.backward()\r\n",
        "                      optimizer.step()\r\n",
        "\r\n",
        "                # statistics(output.argmax(1) == cls)\r\n",
        "                running_loss += loss.item()\r\n",
        "                running_corrects += torch.sum(preds == batch.author)/batch.author.size(0)\r\n",
        "            \r\n",
        "            \r\n",
        "            epoch_loss = running_loss / len(data_loader[phase])\r\n",
        "            epoch_acc = running_corrects / len(data_loader[phase])\r\n",
        "            \r\n",
        "            if phase =='train':\r\n",
        "              \r\n",
        "              train_acc.append(epoch_acc)\r\n",
        "              train_loss.append(epoch_loss)\r\n",
        "            \r\n",
        "            if phase == 'valid':\r\n",
        "              if scheduler:\r\n",
        "                scheduler.step(epoch_loss)\r\n",
        "              valid_acc.append(epoch_acc)\r\n",
        "              valid_loss.append(epoch_loss)\r\n",
        "            \r\n",
        "            print(f'{phase} Loss: {epoch_loss:.5f} Acc: {epoch_acc:.5f}')\r\n",
        "            \r\n",
        "            if phase=='valid':\r\n",
        "              print(f'''last learning rate:{scheduler.state_dict()['_last_lr']}''')\r\n",
        "            \r\n",
        "            \r\n",
        "\r\n",
        "            # deep copy the model\r\n",
        "            if phase == 'valid' and epoch_acc > best_acc:\r\n",
        "                best_acc = epoch_acc\r\n",
        "                #best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "            \r\n",
        "            if phase == 'valid' and epoch_loss < best_loss:\r\n",
        "                best_loss = epoch_loss\r\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "        print()\r\n",
        "\r\n",
        "    time_elapsed = time.time() - since\r\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
        "        time_elapsed // 60, time_elapsed % 60))\r\n",
        "    print('Best validation Acc: {:4f}\\nBest validation Loss : {:4f}'.format(best_acc,best_loss))\r\n",
        " \r\n",
        "    # load best model weights\r\n",
        "    model.load_state_dict(best_model_wts)\r\n",
        "    return model,train_acc,valid_acc,train_loss,valid_loss,time_elapsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fClTvSoS6mGI"
      },
      "source": [
        "# 테스트셋 결과내기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0MZ4Goz6Xp2"
      },
      "source": [
        "def TEST(model_ft,optimizer_ft,model_name):\r\n",
        "  INDEX_LIST=[]\r\n",
        "  PREDICTION_LIST=[]\r\n",
        "  model_ft.eval()\r\n",
        "  for batch in data_loader['test']:\r\n",
        "    text = batch.text.to(device)\r\n",
        "    index = batch.index.to(device) \r\n",
        "    predictions = model_ft(text).to(device)\r\n",
        "    \r\n",
        "    for i,preds in zip(index,predictions):\r\n",
        "      temp=torch.zeros_like(preds)\r\n",
        "      #print(torch.argmax(preds).item())\r\n",
        "      temp[torch.argmax(preds).item()]=1\r\n",
        "      temp = map(int,temp.to(device='cpu').numpy())\r\n",
        "      \r\n",
        "      PREDICTION_LIST.append(temp),INDEX_LIST.append(i.item())\r\n",
        "  submission = pd.DataFrame(PREDICTION_LIST,index=INDEX_LIST)\r\n",
        "  submission.to_csv(f'/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/Submissions/{model_name[:-2]}csv',index_label='index',header=True,encoding='utf-8')\r\n",
        "  return submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmoQpRZW8e8T"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZFAthxLoeGD"
      },
      "source": [
        "## 1) DoubleBiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ui-yr1DoeGK"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class DoubleBiLSTM(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_size, num_classes,drop_out,option,num_layer=2, batch_first=True):\r\n",
        "        super(DoubleBiLSTM, self).__init__()\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.input_size = input_size #embedding size\r\n",
        "        self.num_class = num_classes\r\n",
        "        self.num_layer = num_layer\r\n",
        "        self.option = option\r\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=self.vocab_size, # 워드 임베딩\r\n",
        "                                            embedding_dim=self.input_size)\r\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n",
        "        # with dimensionality hidden_dim.\r\n",
        "        self.lstm = nn.LSTM(self.input_size,self.hidden_size,\r\n",
        "                            num_layers = self.num_layer,\r\n",
        "                            bidirectional = True,\r\n",
        "                            bias=True,\r\n",
        "                            batch_first = True\r\n",
        "                            ,dropout=drop_out\r\n",
        "                            )\r\n",
        "        # The linear layer that maps from hidden state space to tag space\r\n",
        "        self.linear = nn.Linear(self.hidden_size,self.num_class)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        #self.activation = nn.LogSoftmax(dim=-1)\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        output = self.embedding_layer(x)\r\n",
        "        #print(output.shape)\r\n",
        "        #output = batch,embedding_size\r\n",
        "        total, (hidden_last, cell) = self.lstm(output)\r\n",
        "        #total =batch, seq, num_layers * num_directions\r\n",
        "        # hidden.shape=(num_layers * num_directions, batch, hidden_size)\r\n",
        "        #print(hidden_last.shape)\r\n",
        "        hidden_last = hidden_last.view(self.num_layer,2,hidden_last.shape[1],self.hidden_size)\r\n",
        "        hidden_last = hidden_last[-1,:,:,:]\r\n",
        "        \r\n",
        "        if self.option == 'sum':\r\n",
        "          out = hidden_last.sum(dim=0)\r\n",
        "\r\n",
        "        if self.option=='mean':\r\n",
        "          out = hidden_last.mean(dim=0)\r\n",
        "          \r\n",
        "        out = self.linear(out)\r\n",
        "        out = self.relu(out)\r\n",
        "        #out = self.activation(out)\r\n",
        "        #print(out.shape)\r\n",
        "        \r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVVHiRW48q8a"
      },
      "source": [
        "def Train_DoubleBiLSTM(vocab_size,input_size,hidden_dim,batch_size,num_classes,drop_out,num_epochs,option,max_sequence_length,max_grad_norm=False,weight=False):\r\n",
        "  from torch.optim import lr_scheduler\r\n",
        "  from datetime import datetime\r\n",
        "  model_ft=DoubleBiLSTM(vocab_size,input_size,hidden_dim,batch_size,num_classes,drop_out,option).to(device) # (embedding_dim,hidden_dim,vocab_size).to(device)\r\n",
        "  optimizer_ft=torch.optim.SGD(model_ft.parameters(), lr=0.01,momentum=0.9)\r\n",
        "  if max_grad_norm:\r\n",
        "    torch.nn.utils.clip_grad_norm_(model_ft.parameters(),max_grad_norm=5)\r\n",
        "  \r\n",
        "  if weight:\r\n",
        "    criterion = nn.CrossEntropyLoss(torch.tensor(class_weight).to(device,dtype=torch.float)).to(device,dtype=torch.float)\r\n",
        "    model_name = f'{str(today())}_DoubleBiLSTM_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}_weight.pt'\r\n",
        "    json_name = f'{str(today())}_DoubleBiLSTM_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}_weight.json'\r\n",
        "  \r\n",
        "  else:\r\n",
        "    criterion = nn.CrossEntropyLoss().to(device,dtype=torch.float)\r\n",
        "    model_name = f'{str(today())}_DoubleBiLSTM_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}.pt'\r\n",
        "    json_name = f'{str(today())}_DoubleBiLSTM_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}.json'\r\n",
        "  \r\n",
        "  exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min',factor=0.1,patience=int(num_epochs*0.1),\r\n",
        "                                                    threshold=0.01,min_lr=1e-9,eps=1e-09)\r\n",
        "  MODEL_PATH = os.getcwd()+'/Models/'+model_name\r\n",
        "  print(model_name)\r\n",
        "  model_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed = train_model(model_ft, criterion, optimizer_ft,scheduler = exp_lr_scheduler, num_epochs=num_epochs)\r\n",
        "  return model_ft,optimizer_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed,json_name,model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipTLcLjX_Flr"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_T3NjD-5wy",
        "outputId": "09b1ce99-e5de-45a7-d9be-ded3819abb5b"
      },
      "source": [
        "if __name__=='__main__':\r\n",
        "  #data_loader,max_sequence_length,vocab_size = make_dataset_by_tokenizer('spacy',95)\r\n",
        "  model_ft,optimizer_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed,json_name,model_name = Train_DoubleBiLSTM(vocab_size,500,200,256,5,0.5,40,'sum',max_sequence_length,weight=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20210310_DoubleBiLSTM_seqlen_106_500_200_sum.pt\n",
            "Epoch 1/40\n",
            "----------\n",
            "train Loss: 1.56083 Acc: 0.29037\n",
            "valid Loss: 1.55437 Acc: 0.30378\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 2/40\n",
            "----------\n",
            "train Loss: 1.53878 Acc: 0.32873\n",
            "valid Loss: 1.52917 Acc: 0.32227\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 3/40\n",
            "----------\n",
            "train Loss: 1.49806 Acc: 0.37831\n",
            "valid Loss: 1.45594 Acc: 0.40508\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 4/40\n",
            "----------\n",
            "train Loss: 1.40961 Acc: 0.42814\n",
            "valid Loss: 1.35255 Acc: 0.44958\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 5/40\n",
            "----------\n",
            "train Loss: 1.32523 Acc: 0.46636\n",
            "valid Loss: 1.28421 Acc: 0.47881\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 6/40\n",
            "----------\n",
            "train Loss: 1.25218 Acc: 0.49800\n",
            "valid Loss: 1.28059 Acc: 0.48004\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 7/40\n",
            "----------\n",
            "train Loss: 1.18659 Acc: 0.53084\n",
            "valid Loss: 1.19271 Acc: 0.52758\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 8/40\n",
            "----------\n",
            "train Loss: 1.10360 Acc: 0.56738\n",
            "valid Loss: 1.12398 Acc: 0.56177\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 9/40\n",
            "----------\n",
            "train Loss: 1.02496 Acc: 0.60355\n",
            "valid Loss: 1.03193 Acc: 0.59637\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 10/40\n",
            "----------\n",
            "train Loss: 0.94509 Acc: 0.63879\n",
            "valid Loss: 0.99270 Acc: 0.62099\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 11/40\n",
            "----------\n",
            "train Loss: 0.88410 Acc: 0.66587\n",
            "valid Loss: 0.98878 Acc: 0.62295\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 12/40\n",
            "----------\n",
            "train Loss: 0.82989 Acc: 0.68821\n",
            "valid Loss: 0.91274 Acc: 0.65345\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 13/40\n",
            "----------\n",
            "train Loss: 0.77134 Acc: 0.71362\n",
            "valid Loss: 0.88600 Acc: 0.66898\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 14/40\n",
            "----------\n",
            "train Loss: 0.72377 Acc: 0.72814\n",
            "valid Loss: 0.87815 Acc: 0.67099\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 15/40\n",
            "----------\n",
            "train Loss: 0.68935 Acc: 0.74406\n",
            "valid Loss: 0.87756 Acc: 0.67447\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 16/40\n",
            "----------\n",
            "train Loss: 0.64019 Acc: 0.76384\n",
            "valid Loss: 0.91901 Acc: 0.66639\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 17/40\n",
            "----------\n",
            "train Loss: 0.60245 Acc: 0.77874\n",
            "valid Loss: 0.85675 Acc: 0.69404\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 18/40\n",
            "----------\n",
            "train Loss: 0.55343 Acc: 0.79723\n",
            "valid Loss: 0.88289 Acc: 0.69339\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 19/40\n",
            "----------\n",
            "train Loss: 0.52855 Acc: 0.80704\n",
            "valid Loss: 0.87908 Acc: 0.70035\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 20/40\n",
            "----------\n",
            "train Loss: 0.48888 Acc: 0.82271\n",
            "valid Loss: 0.89326 Acc: 0.69918\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 21/40\n",
            "----------\n",
            "train Loss: 0.45678 Acc: 0.83352\n",
            "valid Loss: 0.89037 Acc: 0.70364\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 22/40\n",
            "----------\n",
            "train Loss: 0.43495 Acc: 0.84363\n",
            "valid Loss: 0.97044 Acc: 0.68520\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 23/40\n",
            "----------\n",
            "train Loss: 0.35415 Acc: 0.87527\n",
            "valid Loss: 0.87530 Acc: 0.71596\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 24/40\n",
            "----------\n",
            "train Loss: 0.33585 Acc: 0.88128\n",
            "valid Loss: 0.88184 Acc: 0.71822\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 25/40\n",
            "----------\n",
            "train Loss: 0.32483 Acc: 0.88705\n",
            "valid Loss: 0.89664 Acc: 0.71854\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 26/40\n",
            "----------\n",
            "train Loss: 0.31537 Acc: 0.89041\n",
            "valid Loss: 0.90801 Acc: 0.71737\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 27/40\n",
            "----------\n",
            "train Loss: 0.31398 Acc: 0.88930\n",
            "valid Loss: 0.91953 Acc: 0.71779\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 28/40\n",
            "----------\n",
            "train Loss: 0.30116 Acc: 0.89428\n",
            "valid Loss: 0.91939 Acc: 0.71758\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 29/40\n",
            "----------\n",
            "train Loss: 0.29965 Acc: 0.89546\n",
            "valid Loss: 0.92163 Acc: 0.71817\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 30/40\n",
            "----------\n",
            "train Loss: 0.29922 Acc: 0.89589\n",
            "valid Loss: 0.92301 Acc: 0.71771\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 31/40\n",
            "----------\n",
            "train Loss: 0.29848 Acc: 0.89487\n",
            "valid Loss: 0.92161 Acc: 0.71779\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 32/40\n",
            "----------\n",
            "train Loss: 0.29866 Acc: 0.89596\n",
            "valid Loss: 0.92292 Acc: 0.71817\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 33/40\n",
            "----------\n",
            "train Loss: 0.29552 Acc: 0.89821\n",
            "valid Loss: 0.92339 Acc: 0.71824\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 34/40\n",
            "----------\n",
            "train Loss: 0.29563 Acc: 0.89752\n",
            "valid Loss: 0.92327 Acc: 0.71785\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 35/40\n",
            "----------\n",
            "train Loss: 0.29654 Acc: 0.89764\n",
            "valid Loss: 0.92406 Acc: 0.71789\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 36/40\n",
            "----------\n",
            "train Loss: 0.29675 Acc: 0.89700\n",
            "valid Loss: 0.92421 Acc: 0.71735\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 37/40\n",
            "----------\n",
            "train Loss: 0.29910 Acc: 0.89682\n",
            "valid Loss: 0.92431 Acc: 0.71775\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 38/40\n",
            "----------\n",
            "train Loss: 0.29601 Acc: 0.89662\n",
            "valid Loss: 0.92470 Acc: 0.71770\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 39/40\n",
            "----------\n",
            "train Loss: 0.29612 Acc: 0.89782\n",
            "valid Loss: 0.92457 Acc: 0.71779\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 40/40\n",
            "----------\n",
            "train Loss: 0.29837 Acc: 0.89564\n",
            "valid Loss: 0.92468 Acc: 0.71774\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Training complete in 18m 9s\n",
            "Best validation Acc: 0.718542\n",
            "Best validation Loss : 0.856748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i2pxEpQpi-X",
        "outputId": "0a8a117e-1419-4459-9344-ae5f624ac0b9"
      },
      "source": [
        "SaveModels(model_name)\r\n",
        "SaveJson(json_name,valid_loss,valid_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved Path :/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/Models/20210310_DoubleBiLSTM_seqlen_106_500_200_sum.pt\n",
            "Saving JSON completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dd36mOkWuYqm",
        "outputId": "cb227046-4b64-43c6-da14-5f50d00ad68a"
      },
      "source": [
        "PATH=os.getcwd()+'/'+model_name\r\n",
        "model_ft=DoubleBiLSTM(vocab_size,500,250,256,5,0.5,'mean').to(device) # (embedding_dim,hidden_dim,vocab_size).to(device)\r\n",
        "optimizer_ft=torch.optim.SGD(model_ft.parameters(), lr=0.01,momentum=0.9)\r\n",
        "model_ft,optimizer_ft = LoadModels(PATH,model_ft,optimizer_ft)\r\n",
        "criterion = nn.NLLLoss(torch.tensor(class_weight).to(device,dtype=torch.float)).to(device,dtype=torch.float)\r\n",
        "num_epochs=20\r\n",
        "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min',factor=0.1,patience=int(num_epochs*0.1),\r\n",
        "                                                    threshold=0.01,min_lr=1e-9,eps=1e-09)\r\n",
        "print(model_name)\r\n",
        "\r\n",
        "model_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed = train_model(model_ft, criterion, optimizer_ft,scheduler=exp_lr_scheduler, num_epochs=20)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20210308_DoubleBiLSTM_seqlen_106_500_250_mean_weight.pt\n",
            "Epoch 1/20\n",
            "----------\n",
            "train Loss: 0.57543 Acc: 0.79759\n",
            "valid Loss: 0.87282 Acc: 0.69538\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "train Loss: 0.57755 Acc: 0.79645\n",
            "valid Loss: 0.87139 Acc: 0.69577\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "train Loss: 0.57529 Acc: 0.79546\n",
            "valid Loss: 0.87020 Acc: 0.69540\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "train Loss: 0.57460 Acc: 0.79657\n",
            "valid Loss: 0.86948 Acc: 0.69509\n",
            "last learning rate:[1.0000000000000002e-07]\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "train Loss: 0.57341 Acc: 0.79716\n",
            "valid Loss: 0.86963 Acc: 0.69503\n",
            "last learning rate:[1.0000000000000002e-07]\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "train Loss: 0.57328 Acc: 0.79809\n",
            "valid Loss: 0.86881 Acc: 0.69521\n",
            "last learning rate:[1.0000000000000002e-07]\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "train Loss: 0.57569 Acc: 0.79578\n",
            "valid Loss: 0.86910 Acc: 0.69501\n",
            "last learning rate:[1.0000000000000004e-08]\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "train Loss: 0.57297 Acc: 0.79680\n",
            "valid Loss: 0.86853 Acc: 0.69507\n",
            "last learning rate:[1.0000000000000004e-08]\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "train Loss: 0.57474 Acc: 0.79680\n",
            "valid Loss: 0.86909 Acc: 0.69496\n",
            "last learning rate:[1.0000000000000004e-08]\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "train Loss: 0.57497 Acc: 0.79720\n",
            "valid Loss: 0.86886 Acc: 0.69500\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "train Loss: 0.57479 Acc: 0.79518\n",
            "valid Loss: 0.86926 Acc: 0.69503\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "train Loss: 0.57430 Acc: 0.79659\n",
            "valid Loss: 0.86889 Acc: 0.69505\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "train Loss: 0.57492 Acc: 0.79725\n",
            "valid Loss: 0.86946 Acc: 0.69492\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "train Loss: 0.57347 Acc: 0.79621\n",
            "valid Loss: 0.86891 Acc: 0.69496\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "train Loss: 0.57399 Acc: 0.79686\n",
            "valid Loss: 0.86894 Acc: 0.69503\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "train Loss: 0.57479 Acc: 0.79455\n",
            "valid Loss: 0.86867 Acc: 0.69492\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "train Loss: 0.57559 Acc: 0.79725\n",
            "valid Loss: 0.86851 Acc: 0.69513\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "train Loss: 0.57472 Acc: 0.79778\n",
            "valid Loss: 0.86881 Acc: 0.69509\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "train Loss: 0.57606 Acc: 0.79582\n",
            "valid Loss: 0.86902 Acc: 0.69482\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "train Loss: 0.57519 Acc: 0.79589\n",
            "valid Loss: 0.86868 Acc: 0.69504\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Training complete in 11m 49s\n",
            "Best validation Acc: 0.695767\n",
            "Best validation Loss : 0.868507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-9651be729a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mTEST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-7797caf15c33>\u001b[0m in \u001b[0;36mTEST\u001b[0;34m(model_ft, optimizer_ft, model_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mPREDICTION_LIST\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdW4Smxm_6HO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "d86e8178-fe69-408b-c5d9-443492e51854"
      },
      "source": [
        "TEST(model_ft,optimizer_ft,model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19612</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19613</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19614</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19615</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19616</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19617 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1  2  3  4\n",
              "0      0  1  0  0  0\n",
              "1      0  0  0  0  1\n",
              "2      1  0  0  0  0\n",
              "3      0  0  1  0  0\n",
              "4      0  1  0  0  0\n",
              "...   .. .. .. .. ..\n",
              "19612  0  1  0  0  0\n",
              "19613  0  0  0  0  1\n",
              "19614  0  1  0  0  0\n",
              "19615  0  1  0  0  0\n",
              "19616  0  0  0  1  0\n",
              "\n",
              "[19617 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBGMiImGxXH7"
      },
      "source": [
        "## 2) DoubleBiGRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMfD2SSfxXIB"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class DoubleBiGRU(nn.Module):\r\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_size, num_classes,drop_out,option,num_layers=2):\r\n",
        "        super(DoubleBiGRU, self).__init__()\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.input_size = input_size #embedding size\r\n",
        "        self.num_class = num_classes\r\n",
        "        self.num_layer = num_layers\r\n",
        "        self.dropout=drop_out\r\n",
        "        self.option = option\r\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=self.vocab_size, # 워드 임베딩\r\n",
        "                                            embedding_dim=self.input_size)\r\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n",
        "        # with dimensionality hidden_dim.\r\n",
        "        self.gru = nn.GRU(self.input_size,self.hidden_size,\r\n",
        "                            num_layers = self.num_layer,\r\n",
        "                            bidirectional = True,\r\n",
        "                            bias=True,\r\n",
        "                            batch_first = True,\r\n",
        "                            dropout = self.dropout          \r\n",
        "                            )\r\n",
        "        # The linear layer that maps from hidden state space to tag space\r\n",
        "        self.linear1 = nn.Linear(self.hidden_size,int(0.5*self.hidden_size))\r\n",
        "        self.linear2 = nn.Linear(int(0.5*self.hidden_size),self.num_class)\r\n",
        "        self.dropout = nn.Dropout(0.5)\r\n",
        "        self.activation = nn.LogSoftmax(dim=-1)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear1.weight)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear2.weight)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        output = self.embedding_layer(x)\r\n",
        "        output, h_n = self.gru(output)\r\n",
        "        if self.option == 'sum':\r\n",
        "          out = h_n.sum(dim=0)\r\n",
        "\r\n",
        "        if self.option=='mean':\r\n",
        "          out = h_n.mean(dim=0)\r\n",
        "        #print(h_n.shape)\r\n",
        "        #output : (seq_len, batch, num_directions, hidden_size)\r\n",
        "        #h_n of shape :  (num_layers * num_directions, batch, hidden_size)\r\n",
        "          \r\n",
        "        out = self.linear1(out)\r\n",
        "        out = self.linear2(out)\r\n",
        "        out = self.dropout(out)\r\n",
        "        out = self.activation(out)      \r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfj8oU4LxXID"
      },
      "source": [
        "def Train_DoubleBiGRU(lr_rate,vocab_size,input_size,hidden_dim,batch_size,num_classes,drop_out,option,num_epochs,max_sequence_length,max_grad_norm=False,weight=False):\r\n",
        "  from torch.optim import lr_scheduler\r\n",
        "  from datetime import datetime\r\n",
        "  model_ft=DoubleBiGRU(vocab_size,input_size,hidden_dim,batch_size,num_classes,drop_out,option).to(device)\r\n",
        "  optimizer_ft=torch.optim.SGD(model_ft.parameters(), lr=lr_rate,momentum=0.9)\r\n",
        "  if max_grad_norm:\r\n",
        "    torch.nn.utils.clip_grad_norm_(model_ft.parameters(),max_grad_norm=5)\r\n",
        "  \r\n",
        "  if weight:\r\n",
        "    criterion = nn.NLLLoss(torch.tensor(class_weight).to(device,dtype=torch.float)).to(device,dtype=torch.float)\r\n",
        "    model_name = f'{str(today())}_DoubleGRU_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}_weight.pt'\r\n",
        "    json_name = f'{str(today())}_DoubleBiGRU_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}_weight.json'\r\n",
        "  \r\n",
        "  else:\r\n",
        "    criterion = nn.NLLLoss().to(device,dtype=torch.float)\r\n",
        "    model_name = f'{str(today())}_DoubleBiGRU_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}.pt'\r\n",
        "    json_name = f'{str(today())}_DoubleBiGRU_seqlen_{max_sequence_length}_{str(input_size)}_{str(hidden_dim)}_{option}.json'\r\n",
        "  \r\n",
        "  exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min',factor=0.1,patience=int(num_epochs*0.1),\r\n",
        "                                                    threshold=0.01,min_lr=1e-9,eps=1e-09)\r\n",
        "  MODEL_PATH = os.getcwd()+'/Models/'+model_name\r\n",
        "  print(model_name)\r\n",
        "  model_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed = train_model(model_ft, criterion, optimizer_ft,scheduler = exp_lr_scheduler, num_epochs=num_epochs)\r\n",
        "  \r\n",
        "  return model_ft,optimizer_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed,json_name,model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdkNrvNaxXIE"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVIm-Gg9xXIF",
        "outputId": "816d48f1-c212-4df6-a3ab-3b00b968a479"
      },
      "source": [
        "if __name__=='__main__':\r\n",
        "  data_loader,max_sequence_length,vocab_size = make_dataset_by_tokenizer('spacy',95)\r\n",
        "  model_ft,optimizer_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed,json_name,model_name = Train_DoubleBiGRU(0.01,vocab_size,600,50,256,5,0.1,'sum',40,max_sequence_length,weight=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "20210310_DoubleBiGRU_seqlen_106_600_50_sum.pt\n",
            "Epoch 1/40\n",
            "----------\n",
            "train Loss: 1.64623 Acc: 0.28390\n",
            "valid Loss: 1.49825 Acc: 0.38039\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 2/40\n",
            "----------\n",
            "train Loss: 1.50856 Acc: 0.32973\n",
            "valid Loss: 1.44427 Acc: 0.41279\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 3/40\n",
            "----------\n",
            "train Loss: 1.47506 Acc: 0.34956\n",
            "valid Loss: 1.39817 Acc: 0.44548\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 4/40\n",
            "----------\n",
            "train Loss: 1.43844 Acc: 0.36591\n",
            "valid Loss: 1.35998 Acc: 0.46604\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 5/40\n",
            "----------\n",
            "train Loss: 1.40888 Acc: 0.38482\n",
            "valid Loss: 1.32179 Acc: 0.48570\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 6/40\n",
            "----------\n",
            "train Loss: 1.37178 Acc: 0.39802\n",
            "valid Loss: 1.26727 Acc: 0.50773\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 7/40\n",
            "----------\n",
            "train Loss: 1.34729 Acc: 0.41367\n",
            "valid Loss: 1.23561 Acc: 0.52206\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 8/40\n",
            "----------\n",
            "train Loss: 1.31913 Acc: 0.42396\n",
            "valid Loss: 1.20928 Acc: 0.52946\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 9/40\n",
            "----------\n",
            "train Loss: 1.28148 Acc: 0.44047\n",
            "valid Loss: 1.15689 Acc: 0.55521\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 10/40\n",
            "----------\n",
            "train Loss: 1.24650 Acc: 0.45374\n",
            "valid Loss: 1.12210 Acc: 0.57570\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 11/40\n",
            "----------\n",
            "train Loss: 1.22412 Acc: 0.46398\n",
            "valid Loss: 1.08798 Acc: 0.58542\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 12/40\n",
            "----------\n",
            "train Loss: 1.19324 Acc: 0.47781\n",
            "valid Loss: 1.05017 Acc: 0.60350\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 13/40\n",
            "----------\n",
            "train Loss: 1.16887 Acc: 0.48374\n",
            "valid Loss: 1.02717 Acc: 0.61293\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 14/40\n",
            "----------\n",
            "train Loss: 1.14221 Acc: 0.49787\n",
            "valid Loss: 0.99773 Acc: 0.62526\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 15/40\n",
            "----------\n",
            "train Loss: 1.11634 Acc: 0.50940\n",
            "valid Loss: 0.98140 Acc: 0.63167\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 16/40\n",
            "----------\n",
            "train Loss: 1.09790 Acc: 0.51633\n",
            "valid Loss: 0.95528 Acc: 0.63976\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 17/40\n",
            "----------\n",
            "train Loss: 1.07844 Acc: 0.52255\n",
            "valid Loss: 0.94178 Acc: 0.64388\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 18/40\n",
            "----------\n",
            "train Loss: 1.05572 Acc: 0.53146\n",
            "valid Loss: 0.93077 Acc: 0.64971\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 19/40\n",
            "----------\n",
            "train Loss: 1.03514 Acc: 0.53480\n",
            "valid Loss: 0.91991 Acc: 0.65276\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 20/40\n",
            "----------\n",
            "train Loss: 1.01205 Acc: 0.54847\n",
            "valid Loss: 0.89705 Acc: 0.66380\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 21/40\n",
            "----------\n",
            "train Loss: 1.00108 Acc: 0.55448\n",
            "valid Loss: 0.89115 Acc: 0.66531\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 22/40\n",
            "----------\n",
            "train Loss: 0.97776 Acc: 0.56094\n",
            "valid Loss: 0.87794 Acc: 0.67077\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 23/40\n",
            "----------\n",
            "train Loss: 0.96220 Acc: 0.56725\n",
            "valid Loss: 0.87482 Acc: 0.67531\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 24/40\n",
            "----------\n",
            "train Loss: 0.94982 Acc: 0.56920\n",
            "valid Loss: 0.87117 Acc: 0.67832\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 25/40\n",
            "----------\n",
            "train Loss: 0.93573 Acc: 0.57740\n",
            "valid Loss: 0.86604 Acc: 0.67709\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 26/40\n",
            "----------\n",
            "train Loss: 0.91592 Acc: 0.58251\n",
            "valid Loss: 0.86345 Acc: 0.68113\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 27/40\n",
            "----------\n",
            "train Loss: 0.90629 Acc: 0.58523\n",
            "valid Loss: 0.87336 Acc: 0.67899\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 28/40\n",
            "----------\n",
            "train Loss: 0.89203 Acc: 0.59126\n",
            "valid Loss: 0.85797 Acc: 0.68093\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 29/40\n",
            "----------\n",
            "train Loss: 0.87590 Acc: 0.59580\n",
            "valid Loss: 0.87366 Acc: 0.68346\n",
            "last learning rate:[0.01]\n",
            "\n",
            "Epoch 30/40\n",
            "----------\n",
            "train Loss: 0.86338 Acc: 0.59834\n",
            "valid Loss: 0.87889 Acc: 0.67139\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 31/40\n",
            "----------\n",
            "train Loss: 0.82137 Acc: 0.61256\n",
            "valid Loss: 0.85130 Acc: 0.69041\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 32/40\n",
            "----------\n",
            "train Loss: 0.81603 Acc: 0.61686\n",
            "valid Loss: 0.85616 Acc: 0.69041\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 33/40\n",
            "----------\n",
            "train Loss: 0.81450 Acc: 0.61733\n",
            "valid Loss: 0.85281 Acc: 0.69117\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 34/40\n",
            "----------\n",
            "train Loss: 0.81258 Acc: 0.61601\n",
            "valid Loss: 0.85719 Acc: 0.69138\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 35/40\n",
            "----------\n",
            "train Loss: 0.81353 Acc: 0.61539\n",
            "valid Loss: 0.85631 Acc: 0.69146\n",
            "last learning rate:[0.001]\n",
            "\n",
            "Epoch 36/40\n",
            "----------\n",
            "train Loss: 0.80213 Acc: 0.62123\n",
            "valid Loss: 0.86190 Acc: 0.69046\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 37/40\n",
            "----------\n",
            "train Loss: 0.80164 Acc: 0.61905\n",
            "valid Loss: 0.86012 Acc: 0.69242\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 38/40\n",
            "----------\n",
            "train Loss: 0.80285 Acc: 0.61803\n",
            "valid Loss: 0.86033 Acc: 0.69170\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 39/40\n",
            "----------\n",
            "train Loss: 0.79851 Acc: 0.61928\n",
            "valid Loss: 0.86115 Acc: 0.69243\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 40/40\n",
            "----------\n",
            "train Loss: 0.80196 Acc: 0.62155\n",
            "valid Loss: 0.86126 Acc: 0.69167\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Training complete in 5m 10s\n",
            "Best validation Acc: 0.692431\n",
            "Best validation Loss : 0.851299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOSYuIgGxXIG",
        "outputId": "f9f51d35-bd6c-4902-8d89-c0b09f6a7d14"
      },
      "source": [
        "SaveModels(model_name)\r\n",
        "SaveJson(json_name,valid_loss,valid_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved Path :/gdrive/MyDrive/Colab Notebooks/[DACON]TextClassification/Models/20210310_DoubleBiGRU_seqlen_106_600_50_sum.pt\n",
            "Saving JSON completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0faxdlVxXIG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50af3b23-5381-413f-8793-3264281d4556"
      },
      "source": [
        "PATH=os.getcwd()+'/'+model_name\r\n",
        "model_ft=DoubleBiGRU(vocab_size,600,50,256,5,0.1,'sum').to(device) # (embedding_dim,hidden_dim,vocab_size).to(device)\r\n",
        "optimizer_ft=torch.optim.SGD(model_ft.parameters(), lr=0.01,momentum=0.9)\r\n",
        "model_ft,optimizer_ft = LoadModels(PATH,model_ft,optimizer_ft)\r\n",
        "criterion = nn.NLLLoss().to(device,dtype=torch.float)\r\n",
        "num_epochs=20\r\n",
        "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min',factor=0.1,patience=int(num_epochs*0.1),\r\n",
        "                                                    threshold=0.01,min_lr=1e-9,eps=1e-09)\r\n",
        "  \r\n",
        "print(model_name)\r\n",
        "model_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed = train_model(model_ft, criterion, optimizer_ft,scheduler=exp_lr_scheduler, num_epochs=20)\r\n",
        "TEST(model_ft,optimizer_ft,model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20210310_DoubleBiGRU_seqlen_106_600_50_sum.pt\n",
            "Epoch 1/20\n",
            "----------\n",
            "train Loss: 0.81223 Acc: 0.61946\n",
            "valid Loss: 0.85141 Acc: 0.69000\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "train Loss: 0.81722 Acc: 0.61458\n",
            "valid Loss: 0.85065 Acc: 0.68955\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "train Loss: 0.81347 Acc: 0.61710\n",
            "valid Loss: 0.85169 Acc: 0.69015\n",
            "last learning rate:[0.0001]\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "train Loss: 0.81619 Acc: 0.61701\n",
            "valid Loss: 0.85042 Acc: 0.68986\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "train Loss: 0.80926 Acc: 0.61724\n",
            "valid Loss: 0.85112 Acc: 0.69015\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "train Loss: 0.81390 Acc: 0.61744\n",
            "valid Loss: 0.85148 Acc: 0.69071\n",
            "last learning rate:[1e-05]\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "train Loss: 0.81362 Acc: 0.61785\n",
            "valid Loss: 0.85155 Acc: 0.69065\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "train Loss: 0.81458 Acc: 0.61517\n",
            "valid Loss: 0.85162 Acc: 0.69059\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "train Loss: 0.81148 Acc: 0.61855\n",
            "valid Loss: 0.85108 Acc: 0.69081\n",
            "last learning rate:[1.0000000000000002e-06]\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "train Loss: 0.81322 Acc: 0.61524\n",
            "valid Loss: 0.85183 Acc: 0.69056\n",
            "last learning rate:[1.0000000000000002e-07]\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "train Loss: 0.81429 Acc: 0.61342\n",
            "valid Loss: 0.85169 Acc: 0.69077\n",
            "last learning rate:[1.0000000000000002e-07]\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "train Loss: 0.81675 Acc: 0.61605\n",
            "valid Loss: 0.85135 Acc: 0.69076\n",
            "last learning rate:[1.0000000000000002e-07]\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "train Loss: 0.81370 Acc: 0.61494\n",
            "valid Loss: 0.85156 Acc: 0.69073\n",
            "last learning rate:[1.0000000000000004e-08]\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "train Loss: 0.81306 Acc: 0.61519\n",
            "valid Loss: 0.85112 Acc: 0.69080\n",
            "last learning rate:[1.0000000000000004e-08]\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "train Loss: 0.81229 Acc: 0.61638\n",
            "valid Loss: 0.85100 Acc: 0.69078\n",
            "last learning rate:[1.0000000000000004e-08]\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "train Loss: 0.81316 Acc: 0.61583\n",
            "valid Loss: 0.85113 Acc: 0.69076\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "train Loss: 0.81365 Acc: 0.61424\n",
            "valid Loss: 0.85186 Acc: 0.69061\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "train Loss: 0.80961 Acc: 0.61810\n",
            "valid Loss: 0.85146 Acc: 0.69078\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "train Loss: 0.81224 Acc: 0.61920\n",
            "valid Loss: 0.85157 Acc: 0.69068\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "train Loss: 0.81734 Acc: 0.61365\n",
            "valid Loss: 0.85183 Acc: 0.69060\n",
            "last learning rate:[1.0000000000000005e-09]\n",
            "\n",
            "Training complete in 2m 36s\n",
            "Best validation Acc: 0.690809\n",
            "Best validation Loss : 0.850422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19612</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19613</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19614</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19615</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19616</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19617 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1  2  3  4\n",
              "0      0  1  0  0  0\n",
              "1      1  0  0  0  0\n",
              "2      1  0  0  0  0\n",
              "3      0  0  1  0  0\n",
              "4      0  0  1  0  0\n",
              "...   .. .. .. .. ..\n",
              "19612  0  1  0  0  0\n",
              "19613  0  0  0  0  1\n",
              "19614  0  1  0  0  0\n",
              "19615  0  1  0  0  0\n",
              "19616  0  0  0  0  1\n",
              "\n",
              "[19617 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPlUgCTLprcG"
      },
      "source": [
        "## 3) TransEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUVbW8hCpq4g"
      },
      "source": [
        "class TransEncoder(nn.Module):\r\n",
        "    def __init__(self, vocab_size,max_sequence_length, input_size=512, nhead=8,num_layers=6,num_classes=5, batch_size=256,drop_out=0.5):\r\n",
        "        super(TransEncoder, self).__init__()\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.max_sequence_length = max_sequence_length\r\n",
        "        self.d_model = input_size #embedding size\r\n",
        "        self.nhead = nhead\r\n",
        "        self.num_encoder_layers=num_layers\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.drop_out= drop_out\r\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=self.vocab_size, # 워드 임베딩\r\n",
        "                                            embedding_dim=self.d_model)\r\n",
        "        self.transformer = nn.Transformer(self.d_model,self.nhead,self.num_encoder_layers,dropout=self.drop_out)\r\n",
        "        # The linear layer that maps from hidden state space to tag space\r\n",
        "        self.linear1 = nn.Linear(self.d_model,1)\r\n",
        "        self.linear2 = nn.Linear(self.max_sequence_length,self.num_classes)\r\n",
        "        self.dropout = nn.Dropout(0.5)\r\n",
        "        self.activation = nn.LogSoftmax(dim=-1)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear1.weight)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear2.weight)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        output = self.embedding_layer(x)\r\n",
        "        out = self.transformer.encoder(output)\r\n",
        "        #print(out.shape)  \r\n",
        "        out = self.linear1(out)\r\n",
        "        out = out.view(-1,self.max_sequence_length)\r\n",
        "        out = self.linear2(out)\r\n",
        "        out = self.dropout(out)\r\n",
        "        out = self.activation(out)      \r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PLBKCH03X6J"
      },
      "source": [
        "def Train_TransEncoder(vocab_size,max_sequence_length,input_size, nhead, num_layers, num_classes, batch_size,drop_out,num_epochs,max_grad_norm=False,weight=False):\r\n",
        "  from torch.optim import lr_scheduler\r\n",
        "  from datetime import datetime\r\n",
        "  model_ft=TransEncoder(vocab_size,max_sequence_length,input_size, nhead,num_layers,num_classes,batch_size,drop_out).to(device,dtype=torch.float)\r\n",
        "  optimizer_ft=torch.optim.SGD(model_ft.parameters(), lr=0.01,momentum=0.9)\r\n",
        "  if max_grad_norm:\r\n",
        "    torch.nn.utils.clip_grad_norm_(model_ft.parameters(),max_grad_norm=5)\r\n",
        "  \r\n",
        "  if weight:\r\n",
        "    criterion = nn.NLLLoss(torch.tensor(class_weight).to(device,dtype=torch.float)).to(device,dtype=torch.float)\r\n",
        "    model_name = f'{str(today())}_TransEncoder_seqlen_{max_sequence_length}_{str(input_size)}_nhead_{nhead}_nlayer_{num_layers}_weight.pt'\r\n",
        "    json_name = f'{str(today())}_TransEncoder_seqlen_{max_sequence_length}_{str(input_size)}_nhead_{nhead}_nlayer_{num_layers}_weight.json'\r\n",
        "  \r\n",
        "  else:\r\n",
        "    criterion = nn.NLLLoss().to(device,dtype=torch.float)\r\n",
        "    model_name = f'{str(today())}_TransEncoder_seqlen_{max_sequence_length}_{str(input_size)}_nhead_{nhead}_nlayer_{num_layers}.pt'\r\n",
        "    json_name = f'{str(today())}_TransEncoder_seqlen_{max_sequence_length}_{str(input_size)}_nhead_{nhead}_nlayer_{num_layers}.json'\r\n",
        "  \r\n",
        "  exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min',factor=0.1,patience=int(num_epochs*0.1),\r\n",
        "                                                    threshold=0.01,min_lr=1e-9,eps=1e-09)\r\n",
        "  MODEL_PATH = os.getcwd()+'/Models/'+model_name\r\n",
        "  print(model_name)\r\n",
        "  model_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed = train_model(model_ft, criterion, optimizer_ft,scheduler = exp_lr_scheduler, num_epochs=num_epochs)\r\n",
        "  \r\n",
        "  return model_ft,optimizer_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed,json_name,model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "diSmrseV4oYi",
        "outputId": "f3018175-6581-48d1-e246-7095a50145c8"
      },
      "source": [
        "if __name__=='__main__':\r\n",
        "  data_loader,max_sequence_length,vocab_size = make_dataset_by_tokenizer('spacy',95)\r\n",
        "  model_ft,optimizer_ft,train_acc,valid_acc,train_loss,valid_loss,time_elapsed,json_name,model_name = Train_TransEncoder(vocab_size,max_sequence_length,512,8,9,5,256,0.5,40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "20210310_TransEncoder_seqlen_106_512_nhead_8_nlayer_9.pt\n",
            "Epoch 1/40\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d912800cdc84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset_by_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spacy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain_TransEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-2a16b5e01ca1>\u001b[0m in \u001b[0;36mTrain_TransEncoder\u001b[0;34m(vocab_size, max_sequence_length, input_size, nhead, num_layers, num_classes, batch_size, drop_out, num_epochs, max_grad_norm, weight)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Models/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-49d3600a0a38>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0;31m#print(predictions.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e0e4f24c01e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print(out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m    293\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0;32m--> 294\u001b[0;31m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4802\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4804\u001b[0;31m     \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4805\u001b[0m     \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 14.76 GiB total capacity; 13.65 GiB already allocated; 37.75 MiB free; 13.68 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OqgwS6_AwOt"
      },
      "source": [
        "## 4) Simple Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utTLKD3olkyr"
      },
      "source": [
        "# 모델성능 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "d6-9aK-W9IGr",
        "outputId": "ed90ca6f-3aee-4c80-d2fd-2acd926fdb13"
      },
      "source": [
        "check_df=JsonCheck()\r\n",
        "check_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>Best Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>20210309_DoubleBiGRU_seqlen_106_600_200.pt</td>\n",
              "      <td>0.8015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>20210307_DoubleBiLSTM_seqlen_106_500_200_sum.pt</td>\n",
              "      <td>0.8120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DoubleBiLSTM_500_150_sum.pt</td>\n",
              "      <td>0.8253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20210302_DoubleBiLSTM_seqlen_106_200_256_sum.pt</td>\n",
              "      <td>0.8260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20210307_DoubleBiLSTM_seqlen_106_500_200_sum.pt</td>\n",
              "      <td>0.8286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>20210302_DoubleBiLSTM_seqlen_106_250_128_sum.pt</td>\n",
              "      <td>0.8305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>20210302_DoubleBiLSTM_seqlen_106_250_256_sum.pt</td>\n",
              "      <td>0.8317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20210302_DoubleBiLSTM_seqlen_106_200_512_sum.pt</td>\n",
              "      <td>0.8359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>20210227_DoubleBiLSTM_600_100_sum.pt</td>\n",
              "      <td>0.8361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210227_DoubleBiLSTM_500_150_mean.pt</td>\n",
              "      <td>0.8379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>20210302_DoubleBiLSTM_seqlen_106_200_128_sum.pt</td>\n",
              "      <td>0.8411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>20210301_DoubleBiLSTM_seqlen_106_400_300_sum.pt</td>\n",
              "      <td>0.8514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DoubleBiLSTM_500_150_mean.pt</td>\n",
              "      <td>0.8543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DoubleBiLSTM_500_150_mean.pt</td>\n",
              "      <td>0.8543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>20210302_DoubleBiLSTM_seqlen_106_200_64_sum.pt</td>\n",
              "      <td>0.8577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20210302_DoubleBiLSTM_seqlen_106_250_64_sum.pt</td>\n",
              "      <td>0.8623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20210307_DoubleBiLSTM_seqlen_154_450_200_sum.pt</td>\n",
              "      <td>0.8678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>20210308_DoubleBiLSTM_seqlen_106_500_250_mean_...</td>\n",
              "      <td>0.8738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210227_DoubleBiLSTM_600_100_mean_classweight.pt</td>\n",
              "      <td>0.8769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20210227_DoubleBiLSTM_600_100_sum_classweight.pt</td>\n",
              "      <td>0.8796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210227_DoubleBiLSTM_500_150_mean_classweight.pt</td>\n",
              "      <td>0.8796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>20210227_DoubleBiLSTM_450_200_sum.pt</td>\n",
              "      <td>0.8856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>20210301_DoubleBiLSTM_seqlen_106_200_200_sum.pt</td>\n",
              "      <td>0.9080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 name  Best Loss\n",
              "22         20210309_DoubleBiGRU_seqlen_106_600_200.pt     0.8015\n",
              "18    20210307_DoubleBiLSTM_seqlen_106_500_200_sum.pt     0.8120\n",
              "0                         DoubleBiLSTM_500_150_sum.pt     0.8253\n",
              "13    20210302_DoubleBiLSTM_seqlen_106_200_256_sum.pt     0.8260\n",
              "19    20210307_DoubleBiLSTM_seqlen_106_500_200_sum.pt     0.8286\n",
              "16    20210302_DoubleBiLSTM_seqlen_106_250_128_sum.pt     0.8305\n",
              "17    20210302_DoubleBiLSTM_seqlen_106_250_256_sum.pt     0.8317\n",
              "14    20210302_DoubleBiLSTM_seqlen_106_200_512_sum.pt     0.8359\n",
              "6                20210227_DoubleBiLSTM_600_100_sum.pt     0.8361\n",
              "4               20210227_DoubleBiLSTM_500_150_mean.pt     0.8379\n",
              "12    20210302_DoubleBiLSTM_seqlen_106_200_128_sum.pt     0.8411\n",
              "10    20210301_DoubleBiLSTM_seqlen_106_400_300_sum.pt     0.8514\n",
              "7                        DoubleBiLSTM_500_150_mean.pt     0.8543\n",
              "1                        DoubleBiLSTM_500_150_mean.pt     0.8543\n",
              "11     20210302_DoubleBiLSTM_seqlen_106_200_64_sum.pt     0.8577\n",
              "15     20210302_DoubleBiLSTM_seqlen_106_250_64_sum.pt     0.8623\n",
              "20    20210307_DoubleBiLSTM_seqlen_154_450_200_sum.pt     0.8678\n",
              "21  20210308_DoubleBiLSTM_seqlen_106_500_250_mean_...     0.8738\n",
              "2   20210227_DoubleBiLSTM_600_100_mean_classweight.pt     0.8769\n",
              "5    20210227_DoubleBiLSTM_600_100_sum_classweight.pt     0.8796\n",
              "3   20210227_DoubleBiLSTM_500_150_mean_classweight.pt     0.8796\n",
              "8                20210227_DoubleBiLSTM_450_200_sum.pt     0.8856\n",
              "9     20210301_DoubleBiLSTM_seqlen_106_200_200_sum.pt     0.9080"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIOolbkL1NVV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}