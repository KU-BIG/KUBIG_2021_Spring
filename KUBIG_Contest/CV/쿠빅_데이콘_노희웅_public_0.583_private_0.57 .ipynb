{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "쿠빅_데이콘_노희웅.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79vyWXLGxZPJ",
        "outputId": "3c37c308-e56f-45b9-8509-fa3af3afb665"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1yZRIYtxmPa",
        "outputId": "304ec99d-13c7-488d-e7bb-90ae239a2d21"
      },
      "source": [
        "from google.colab import output\r\n",
        "# !cp 파일1 파일2 # 파일1을 파일2로 복사 붙여넣기\r\n",
        "!cp \"/content/drive/Shareddrives/Practical_Computer_Vision/data_2.zip\" \"data_2.zip\"\r\n",
        "# data_2.zip을 현재 디렉터리에 압축해제\r\n",
        "!unzip \"data_2.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data_2.zip\n",
            "  inflating: dirty_mnist_2nd.zip     \n",
            "  inflating: dirty_mnist_2nd_answer.csv  \n",
            "  inflating: mnist_data.zip          \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_dirty_mnist_2nd.zip  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJzdfrsb8qCT"
      },
      "source": [
        "from google.colab import output\r\n",
        "# 현재 디렉터리에 dirty_mnist라는 폴더 생성\r\n",
        "!mkdir \"./dirty_mnist\"\r\n",
        "#dirty_mnist.zip라는 zip파일을 dirty_mnist라는 폴더에 압축 풀기\r\n",
        "!unzip \"dirty_mnist_2nd.zip\" -d \"./dirty_mnist/\"\r\n",
        "# 현재 디렉터리에 test_dirty_mnist라는 폴더 생성\r\n",
        "!mkdir \"./test_dirty_mnist\"\r\n",
        "#test_dirty_mnist.zip라는 zip파일을 test_dirty_mnist라는 폴더에 압축 풀기\r\n",
        "!unzip \"test_dirty_mnist_2nd.zip\" -d \"./test_dirty_mnist/\"\r\n",
        "# 출력 결과 지우기\r\n",
        "output.clear()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT-IPSkePKkl"
      },
      "source": [
        "#using baseline code pytorch\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm\r\n",
        "import imutils\r\n",
        "import zipfile\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.models as models\r\n",
        "import torchvision.transforms as T\r\n",
        "from torch.utils.data import DataLoader, Dataset\r\n",
        "from google.colab import output\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMoCjg-186QE"
      },
      "source": [
        "dirty_mnist_answer = pd.read_csv(\"dirty_mnist_2nd_answer.csv\")\r\n",
        "\r\n",
        "namelist = os.listdir('./dirty_mnist/')\r\n",
        "\r\n",
        "class ToTensor(object):\r\n",
        "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\r\n",
        "    def __call__(self, sample):\r\n",
        "        image, label = sample['image'], sample['label']\r\n",
        "        image = image.transpose((2, 0, 1))\r\n",
        "        return {'image': torch.FloatTensor(image),\r\n",
        "                'label': torch.FloatTensor(label)}\r\n",
        "to_tensor = T.Compose([\r\n",
        "                        ToTensor()\r\n",
        "                    ])\r\n",
        "\r\n",
        "class DatasetMNIST(torch.utils.data.Dataset):\r\n",
        "    def __init__(self,\r\n",
        "                 dir_path,\r\n",
        "                 meta_df,\r\n",
        "                 transforms=to_tensor,\r\n",
        "                 augmentations=None):\r\n",
        "        \r\n",
        "        self.dir_path = dir_path \r\n",
        "        self.meta_df = meta_df \r\n",
        "\r\n",
        "        self.transforms = transforms\r\n",
        "        self.augmentations = augmentations \r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.meta_df)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        \r\n",
        "        image = cv2.imread(self.dir_path +\\\r\n",
        "                           str(self.meta_df.iloc[index,0]).zfill(5) + '.png',        # 0 ~ 255의 값을 갖고 크기가 (256,256)인 numpy array를\r\n",
        "                           cv2.IMREAD_GRAYSCALE)                                    # 0 ~ 1 사이의 실수를 갖고 크기가 (256,256,1)인 numpy array로 변환\r\n",
        "        image = (image/255).astype('float')[..., np.newaxis]\r\n",
        "        label = self.meta_df.iloc[index, 1:].values.astype('float') # 정답 numpy array생성(존재하면 1 없으면 0)\r\n",
        "        sample = {'image': image, 'label': label}\r\n",
        "        if self.transforms:\r\n",
        "            sample = self.transforms(sample)\r\n",
        "        return sample"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mu3S6GsKSup"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #GPU 설정"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfrSwGvt9DcG",
        "outputId": "0abb499e-b0f5-4abe-d8db-94416c67f6fd"
      },
      "source": [
        "\r\n",
        "class MultiLabelResnet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MultiLabelResnet, self).__init__()\r\n",
        "        self.conv2d = nn.Conv2d(1, 3, 3, stride=1)\r\n",
        "        self.resnet = models.resnet101() \r\n",
        "        self.FC = nn.Linear(1000, 26)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        x = F.relu(self.conv2d(x))\r\n",
        "        x = F.relu(self.resnet(x))\r\n",
        "        x = torch.sigmoid(self.FC(x))\r\n",
        "        return x\r\n",
        "model = MultiLabelResnet()\r\n",
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelResnet(\n",
              "  (conv2d): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (resnet): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (6): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (7): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (8): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (9): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (10): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (11): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (12): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (13): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (14): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (15): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (16): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (17): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (18): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (19): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (20): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (21): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (22): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "  )\n",
              "  (FC): Linear(in_features=1000, out_features=26, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9o_09Dt9KxI",
        "outputId": "ec7060ec-36f2-48f3-a6f3-da26235079dd"
      },
      "source": [
        "# cross validation을 적용하기 위해 KFold 생성\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\r\n",
        "\r\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\r\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\r\n",
        "    print(f'[fold: {fold_index}]')\r\n",
        "    torch.cuda.empty_cache()\r\n",
        "\r\n",
        "    #train fold, validation fold 분할\r\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\r\n",
        "    test_answer  = dirty_mnist_answer.iloc[val_idx]\r\n",
        "\r\n",
        "    #Dataset 정의\r\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist/\", train_answer)\r\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist/\", test_answer)\r\n",
        "\r\n",
        "    #DataLoader 정의\r\n",
        "    train_data_loader = DataLoader(\r\n",
        "        train_dataset,\r\n",
        "        batch_size = 64,\r\n",
        "        shuffle = False,\r\n",
        "        num_workers = 3\r\n",
        "    )\r\n",
        "    valid_data_loader = DataLoader(\r\n",
        "        valid_dataset,\r\n",
        "        batch_size = 32,\r\n",
        "        shuffle = False,\r\n",
        "        num_workers = 3\r\n",
        "    )\r\n",
        "\r\n",
        "    # 모델 선언\r\n",
        "    model = MultiLabelResnet()\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\r\n",
        "                                lr = 0.001)\r\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\r\n",
        "                                                step_size = 5,\r\n",
        "                                                gamma = 0.75)\r\n",
        "    criterion = torch.nn.BCELoss()\r\n",
        "\r\n",
        "    valid_acc_max = 0\r\n",
        "    for epoch in range(5):\r\n",
        "        train_acc_list = []\r\n",
        "        with tqdm(train_data_loader,\r\n",
        "                total=train_data_loader.__len__(), \r\n",
        "                unit=\"batch\") as train_bar:\r\n",
        "            for sample in train_bar:\r\n",
        "                train_bar.set_description(f\"Train Epoch {epoch}\")\r\n",
        "                optimizer.zero_grad()\r\n",
        "                images, labels = sample['image'], sample['label'] \r\n",
        "                images = images.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "                model.train()\r\n",
        "                with torch.set_grad_enabled(True):\r\n",
        "                    \r\n",
        "                    probs  = model(images)\r\n",
        "                    \r\n",
        "                    loss = criterion(probs, labels)\r\n",
        "                    \r\n",
        "                    loss.backward()\r\n",
        "                    \r\n",
        "                    optimizer.step()\r\n",
        "\r\n",
        "                   \r\n",
        "                    probs  = probs.cpu().detach().numpy()\r\n",
        "                    labels = labels.cpu().detach().numpy()\r\n",
        "                    preds = probs > 0.5\r\n",
        "                    batch_acc = (labels == preds).mean()\r\n",
        "                    train_acc_list.append(batch_acc)\r\n",
        "                    train_acc = np.mean(train_acc_list)\r\n",
        "\r\n",
        "               \r\n",
        "                train_bar.set_postfix(train_loss= loss.item(),\r\n",
        "                                      train_acc = train_acc)\r\n",
        "                \r\n",
        "\r\n",
        "    #   Validation acc 계산\r\n",
        "        valid_acc_list = []\r\n",
        "        with tqdm(valid_data_loader,\r\n",
        "                total=valid_data_loader.__len__(),\r\n",
        "                unit=\"batch\") as valid_bar:\r\n",
        "            for sample in valid_bar:\r\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\r\n",
        "                optimizer.zero_grad()\r\n",
        "                images, labels = sample['image'], sample['label']\r\n",
        "                images = images.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "\r\n",
        "                \r\n",
        "                model.eval()\r\n",
        "                \r\n",
        "                with torch.no_grad():\r\n",
        "                    probs  = model(images)\r\n",
        "                    valid_loss = criterion(probs, labels)\r\n",
        "                    probs  = probs.cpu().detach().numpy()\r\n",
        "                    labels = labels.cpu().detach().numpy()\r\n",
        "                    preds = probs > 0.5\r\n",
        "                    batch_acc = (labels == preds).mean()\r\n",
        "                    valid_acc_list.append(batch_acc)\r\n",
        "                valid_acc = np.mean(valid_acc_list)\r\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss.item(),\r\n",
        "                                      valid_acc = valid_acc)\r\n",
        "            \r\n",
        "        lr_scheduler.step()\r\n",
        "        # save model\r\n",
        "        if valid_acc_max < valid_acc:\r\n",
        "            valid_acc_max = valid_acc\r\n",
        "            best_model = model\r\n",
        "            MODEL = \"resnet101\"\r\n",
        "            path = \"/content/\"\r\n",
        "            torch.save(best_model, f'{path}{fold_index}_{MODEL}_{valid_loss.item():2.4f}_epoch_{epoch}.pth')\r\n",
        "\r\n",
        "    # 폴드별로 가장 좋은 모델 저장\r\n",
        "    best_models.append(best_model)\r\n",
        "\r\n",
        "#test Dataset 정의\r\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\r\n",
        "test_dataset = DatasetMNIST(\"test_dirty_mnist/\", sample_submission)\r\n",
        "batch_size = 128\r\n",
        "test_data_loader = DataLoader(\r\n",
        "    test_dataset,\r\n",
        "    batch_size = batch_size,\r\n",
        "    shuffle = False,\r\n",
        "    num_workers = 3,\r\n",
        "    drop_last = False\r\n",
        ")\r\n",
        "predictions_list = []\r\n",
        "prediction_df = pd.read_csv(\"sample_submission.csv\")\r\n",
        "\r\n",
        "# 5개의 fold마다 가장 좋은 모델을 이용하여 예측\r\n",
        "for model in best_models:\r\n",
        "    prediction_array = np.zeros([prediction_df.shape[0],\r\n",
        "                                 prediction_df.shape[1] -1])\r\n",
        "    for idx, sample in enumerate(test_data_loader):\r\n",
        "        with torch.no_grad():\r\n",
        "            model.eval()\r\n",
        "            images = sample['image']\r\n",
        "            images = images.to(device)\r\n",
        "            probs  = model(images)\r\n",
        "            probs = probs.cpu().detach().numpy()\r\n",
        "            preds = (probs > 0.5)\r\n",
        "\r\n",
        "            batch_index = batch_size * idx\r\n",
        "            prediction_array[batch_index: batch_index + images.shape[0],:]\\\r\n",
        "                         = preds.astype(int)\r\n",
        "                         \r\n",
        "    \r\n",
        "    predictions_list.append(prediction_array[...,np.newaxis])\r\n",
        "predictions_array = np.concatenate(predictions_list, axis = 2)\r\n",
        "predictions_mean = predictions_array.mean(axis = 2)\r\n",
        "\r\n",
        "# 평균 값이 0.5보다 클 경우 1 작으면 0\r\n",
        "predictions_mean = (predictions_mean > 0.5) * 1\r\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\r\n",
        "sample_submission.iloc[:,1:] = predictions_mean\r\n",
        "sample_submission.to_csv(\"prac_submission2.csv\", index = False)\r\n",
        "sample_submission.to_csv(\"/content/drive/MyDrive/Ku_Big/prac_submission2.csv\",index=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[fold: 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Train Epoch 0: 100%|██████████| 625/625 [13:55<00:00,  1.34s/batch, train_acc=0.538, train_loss=0.69]\n",
            "Valid Epoch 0: 100%|██████████| 313/313 [01:10<00:00,  4.44batch/s, valid_acc=0.541, valid_loss=0.688]\n",
            "Train Epoch 1: 100%|██████████| 625/625 [14:15<00:00,  1.37s/batch, train_acc=0.544, train_loss=0.688]\n",
            "Valid Epoch 1: 100%|██████████| 313/313 [01:10<00:00,  4.43batch/s, valid_acc=0.538, valid_loss=0.686]\n",
            "Train Epoch 2: 100%|██████████| 625/625 [14:13<00:00,  1.37s/batch, train_acc=0.551, train_loss=0.684]\n",
            "Valid Epoch 2: 100%|██████████| 313/313 [01:10<00:00,  4.45batch/s, valid_acc=0.556, valid_loss=0.684]\n",
            "Train Epoch 3: 100%|██████████| 625/625 [14:11<00:00,  1.36s/batch, train_acc=0.565, train_loss=0.675]\n",
            "Valid Epoch 3: 100%|██████████| 313/313 [01:10<00:00,  4.46batch/s, valid_acc=0.548, valid_loss=0.679]\n",
            "Train Epoch 4: 100%|██████████| 625/625 [14:11<00:00,  1.36s/batch, train_acc=0.574, train_loss=0.667]\n",
            "Valid Epoch 4: 100%|██████████| 313/313 [01:10<00:00,  4.46batch/s, valid_acc=0.558, valid_loss=0.672]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[fold: 2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 0: 100%|██████████| 625/625 [14:13<00:00,  1.37s/batch, train_acc=0.541, train_loss=0.69]\n",
            "Valid Epoch 0: 100%|██████████| 313/313 [01:10<00:00,  4.45batch/s, valid_acc=0.526, valid_loss=0.694]\n",
            "Train Epoch 1: 100%|██████████| 625/625 [14:12<00:00,  1.36s/batch, train_acc=0.552, train_loss=0.684]\n",
            "Valid Epoch 1: 100%|██████████| 313/313 [01:10<00:00,  4.45batch/s, valid_acc=0.55, valid_loss=0.688]\n",
            "Train Epoch 2: 100%|██████████| 625/625 [14:12<00:00,  1.36s/batch, train_acc=0.567, train_loss=0.675]\n",
            "Valid Epoch 2: 100%|██████████| 313/313 [01:09<00:00,  4.47batch/s, valid_acc=0.55, valid_loss=0.723]\n",
            "Train Epoch 3: 100%|██████████| 625/625 [14:12<00:00,  1.36s/batch, train_acc=0.576, train_loss=0.667]\n",
            "Valid Epoch 3: 100%|██████████| 313/313 [01:10<00:00,  4.46batch/s, valid_acc=0.553, valid_loss=0.718]\n",
            "Train Epoch 4: 100%|██████████| 625/625 [14:12<00:00,  1.36s/batch, train_acc=0.59, train_loss=0.659]\n",
            "Valid Epoch 4: 100%|██████████| 313/313 [01:10<00:00,  4.46batch/s, valid_acc=0.57, valid_loss=0.683]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[fold: 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 0: 100%|██████████| 625/625 [14:12<00:00,  1.36s/batch, train_acc=0.538, train_loss=0.688]\n",
            "Valid Epoch 0: 100%|██████████| 313/313 [01:10<00:00,  4.43batch/s, valid_acc=0.539, valid_loss=0.694]\n",
            "Train Epoch 1: 100%|██████████| 625/625 [14:14<00:00,  1.37s/batch, train_acc=0.543, train_loss=0.688]\n",
            "Valid Epoch 1: 100%|██████████| 313/313 [01:10<00:00,  4.43batch/s, valid_acc=0.544, valid_loss=0.697]\n",
            "Train Epoch 2: 100%|██████████| 625/625 [14:15<00:00,  1.37s/batch, train_acc=0.547, train_loss=0.684]\n",
            "Valid Epoch 2: 100%|██████████| 313/313 [01:10<00:00,  4.44batch/s, valid_acc=0.533, valid_loss=0.694]\n",
            "Train Epoch 3: 100%|██████████| 625/625 [14:15<00:00,  1.37s/batch, train_acc=0.56, train_loss=0.676]\n",
            "Valid Epoch 3: 100%|██████████| 313/313 [01:10<00:00,  4.45batch/s, valid_acc=0.56, valid_loss=0.696]\n",
            "Train Epoch 4: 100%|██████████| 625/625 [14:15<00:00,  1.37s/batch, train_acc=0.571, train_loss=0.666]\n",
            "Valid Epoch 4: 100%|██████████| 313/313 [01:10<00:00,  4.44batch/s, valid_acc=0.558, valid_loss=0.686]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[fold: 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 0: 100%|██████████| 625/625 [14:17<00:00,  1.37s/batch, train_acc=0.538, train_loss=0.689]\n",
            "Valid Epoch 0: 100%|██████████| 313/313 [01:10<00:00,  4.42batch/s, valid_acc=0.541, valid_loss=0.689]\n",
            "Train Epoch 1: 100%|██████████| 625/625 [14:16<00:00,  1.37s/batch, train_acc=0.545, train_loss=0.686]\n",
            "Valid Epoch 1: 100%|██████████| 313/313 [01:10<00:00,  4.44batch/s, valid_acc=0.509, valid_loss=0.715]\n",
            "Train Epoch 2: 100%|██████████| 625/625 [14:13<00:00,  1.37s/batch, train_acc=0.558, train_loss=0.677]\n",
            "Valid Epoch 2: 100%|██████████| 313/313 [01:10<00:00,  4.44batch/s, valid_acc=0.548, valid_loss=0.699]\n",
            "Train Epoch 3: 100%|██████████| 625/625 [14:14<00:00,  1.37s/batch, train_acc=0.57, train_loss=0.671]\n",
            "Valid Epoch 3: 100%|██████████| 313/313 [01:10<00:00,  4.43batch/s, valid_acc=0.555, valid_loss=0.701]\n",
            "Train Epoch 4: 100%|██████████| 625/625 [14:13<00:00,  1.37s/batch, train_acc=0.579, train_loss=0.665]\n",
            "Valid Epoch 4: 100%|██████████| 313/313 [01:10<00:00,  4.44batch/s, valid_acc=0.555, valid_loss=0.68]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[fold: 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 0: 100%|██████████| 625/625 [14:18<00:00,  1.37s/batch, train_acc=0.538, train_loss=0.689]\n",
            "Valid Epoch 0: 100%|██████████| 313/313 [01:10<00:00,  4.41batch/s, valid_acc=0.54, valid_loss=0.69]\n",
            "Train Epoch 1: 100%|██████████| 625/625 [14:17<00:00,  1.37s/batch, train_acc=0.542, train_loss=0.689]\n",
            "Valid Epoch 1: 100%|██████████| 313/313 [01:10<00:00,  4.43batch/s, valid_acc=0.538, valid_loss=0.689]\n",
            "Train Epoch 2: 100%|██████████| 625/625 [14:16<00:00,  1.37s/batch, train_acc=0.546, train_loss=0.685]\n",
            "Valid Epoch 2: 100%|██████████| 313/313 [01:10<00:00,  4.45batch/s, valid_acc=0.533, valid_loss=0.68]\n",
            "Train Epoch 3: 100%|██████████| 625/625 [14:12<00:00,  1.36s/batch, train_acc=0.558, train_loss=0.678]\n",
            "Valid Epoch 3: 100%|██████████| 313/313 [01:10<00:00,  4.44batch/s, valid_acc=0.554, valid_loss=0.69]\n",
            "Train Epoch 4: 100%|██████████| 625/625 [14:10<00:00,  1.36s/batch, train_acc=0.566, train_loss=0.673]\n",
            "Valid Epoch 4: 100%|██████████| 313/313 [01:10<00:00,  4.47batch/s, valid_acc=0.569, valid_loss=0.673]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}